{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks for Complete Intersection Calabi-Yau Manifold\n",
    "\n",
    "In the framework of String Theory, we apply **deep learning** techniques for the prediction of the **Hodge numbers** of _Complete Intersection Calabi-Yau_ (CICY) 3-folds. The relevant quantities are therefore $h_{11}$ and $h_{21}$ which can be predicted starting from the configuration matrices of known manifolds.\n",
    "\n",
    "We take advantage of previously studied and feature engineered datasets to build **deep neural networks** (DNN) and **convolutional neural networks** (CNN) to predict the labels. We use [Tensorflow](https://www.tensorflow.org/) as backend framework and its high level API [Keras](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We first setup the environment and import relevant packages which we will use in the analysis. We print their versions to keep track of changes and set the **random seed** of all random generators in order to get reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7\n",
      "Matplot version: 3.2.1\n",
      "Numpy version: 1.18.1\n",
      "Pandas version: 1.0.3\n",
      "Scikit-learn version: 0.22.2.post1\n",
      "Scikit-optimize version: 0.7.4\n",
      "Tensorflow version: 2.0.0\n",
      "Keras version: 2.2.4-tf (backend: tensorflow)\n",
      "XGBoost version: 0.90\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random            as rnd\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import sklearn           as skl\n",
    "import skopt             as sko\n",
    "import tensorflow        as tf\n",
    "import xgboost           as xgb\n",
    "\n",
    "from tensorflow       import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning) # ignore UserWarning: I cannot really do anything about it...\n",
    "\n",
    "print('Python version: {:d}.{:d}'      .format(sys.version_info.major, sys.version_info.minor), flush=True)\n",
    "print('Matplot version: {}'            .format(mpl.__version__),                                flush=True)\n",
    "print('Numpy version: {}'              .format(np.__version__),                                 flush=True)\n",
    "print('Pandas version: {}'             .format(pd.__version__),                                 flush=True)\n",
    "print('Scikit-learn version: {}'       .format(skl.__version__),                                flush=True)\n",
    "print('Scikit-optimize version: {}'    .format(sko.__version__),                                flush=True)\n",
    "print('Tensorflow version: {}'         .format(tf.__version__),                                 flush=True)\n",
    "print('Keras version: {} (backend: {})'.format(keras.__version__, K.backend()),                 flush=True)\n",
    "print('XGBoost version: {}'            .format(xgb.__version__),                                flush=True)\n",
    "\n",
    "# fix random_seed\n",
    "RAND = 42\n",
    "rnd.seed(RAND)\n",
    "np.random.seed(RAND)\n",
    "tf.random.set_seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also print the **hardware specifications** to have a representation of the current build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  GNU/Linux - Arch Linux\n",
      "CPU: Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n",
      "RAM: 8 GiB\n",
      "GPU: NVIDIA Corporation GM108M [GeForce 940MX] (rev ff)\n"
     ]
    }
   ],
   "source": [
    "!echo \"OS:  $(uname -o) - $(lsb_release -d| sed 's/^.*:\\s*//g')\"\n",
    "!echo \"CPU: $(lscpu| grep 'Model name'| sed 's/^.*:\\s*//g')\"\n",
    "!echo \"RAM: $(free --giga| awk '/^Mem/ {print $7}') GiB\"\n",
    "!echo \"GPU: $(lspci | grep '3D controller' | sed 's/^.*controller:\\s*//g')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store information and results, we create a **logger** for the current Python session and a function to print information to the log file (or the standard output, if the logger is not defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from os import path, rename\n",
    "from time import strftime, gmtime\n",
    "\n",
    "def create_logfile(filename, name='logger', level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Create a logfile and rotate old logs.\n",
    "    \n",
    "    Required arguments:\n",
    "        filename: the name of the file or path to the log\n",
    "        \n",
    "    Optional arguments\n",
    "        name:     the name of the log session\n",
    "        level:    the level of the information stores\n",
    "        \n",
    "    Returns:\n",
    "        the log\n",
    "    \"\"\"\n",
    "    \n",
    "    # get current time to rename strings\n",
    "    ctime = strftime('.%Y%m%d.%H%M%S', gmtime())\n",
    "    \n",
    "    # rotate log if it already exists\n",
    "    if path.isfile(filename):\n",
    "        print('Rotating existing logs...', flush=True)\n",
    "        rename(filename, filename + ctime)\n",
    "    \n",
    "    # get a logging session by name\n",
    "    log = logging.getLogger(name + ctime)\n",
    "    log.setLevel(level)\n",
    "    \n",
    "    # define format\n",
    "    fmt = logging.Formatter('%(asctime)s --> %(levelname)s: %(message)s')\n",
    "    \n",
    "    # add the log file\n",
    "    han = logging.FileHandler(filename=filename)\n",
    "    han.setLevel(level)\n",
    "    han.setFormatter(fmt)\n",
    "    \n",
    "    # add handler for standard output\n",
    "    std = logging.StreamHandler(sys.stdout)\n",
    "    std.setLevel(level)\n",
    "    std.setFormatter(fmt)\n",
    "    \n",
    "    # create the output\n",
    "    log.addHandler(han)\n",
    "    log.addHandler(std)\n",
    "    \n",
    "    print('Created new log file!', flush=True)\n",
    "    return log\n",
    "\n",
    "def logprint(string, stream='info', logger=None):\n",
    "    \"\"\"\n",
    "    Decides whether to print on the logger or the standard output.\n",
    "    \n",
    "    Required arguments:\n",
    "        string: the string to print\n",
    "    \n",
    "    Optional arguments:\n",
    "        stream: standard input (info) or standard error (error)\n",
    "        logger: the logger (None for standard output/error)\n",
    "    \"\"\"\n",
    "    \n",
    "    if logger is not None:\n",
    "        if stream == 'info':\n",
    "            logger.info(string)\n",
    "        elif stream == 'error':\n",
    "            logger.error(string)\n",
    "        else:\n",
    "            logger.debug(string)\n",
    "    else:\n",
    "        if stream == 'info':\n",
    "            sys.stdout.write(string)\n",
    "        elif stream == 'error':\n",
    "            sys.stderr.write(string)\n",
    "        else:\n",
    "            sys.stdout.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Tools\n",
    "\n",
    "We first fetch the desired dataset and prepare the tools for the analysis. Specifically we need to:\n",
    "\n",
    "1. define the names of the main **directories** and create them if non existent,\n",
    "2. import the **database** and read the archive,\n",
    "3. create tools for **visualisation** and **manipulation** of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new log file!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from os import makedirs\n",
    "\n",
    "ROOT_DIR = '.'      # root directory\n",
    "IMG_DIR  = 'img'    # image directory\n",
    "MOD_DIR  = 'models' # directory of saved models\n",
    "LOG_DIR  = 'log'    # directory for logs\n",
    "\n",
    "# name of the dataset to be considered\n",
    "DB_NAME = 'cicy3o'\n",
    "DB_FILE = DB_NAME + '.h5'                                     # full name with extension\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE)                        # full path\n",
    "DB_DIR  = 'original' if DB_NAME == 'cicy3o' else 'favourable' # subdir where to store images, models, logs\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR, DB_DIR)\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR, DB_DIR)\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR, DB_DIR)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok=True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok=True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok=True)\n",
    "    \n",
    "# create logfile\n",
    "logger = create_logfile(filename=path.join(LOG_PATH, DB_NAME + '_nn.log'), name='CICY3', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then prepare to load the dataset and prepare for the visualisation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(filepath, mode='hdf5', shuffle=False, random_state=None, logger=None):\n",
    "    \"\"\"\n",
    "    Load a dataset given the path and the format.\n",
    "    \n",
    "    Required arguments:\n",
    "        filepath: the path of the file\n",
    "        \n",
    "    Optional arguments:\n",
    "        mode:         the format of the file\n",
    "        shuffle:      whether to shuffle the file\n",
    "        random_state: the seed of the random generator\n",
    "        logger:       the logging session (None for standard output)\n",
    "        \n",
    "    Returns:\n",
    "        the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if path.isfile(filepath):\n",
    "        logprint('Reading database...', logger=logger)\n",
    "        if mode == 'hdf5':\n",
    "            df = pd.read_hdf(filepath)\n",
    "        elif mode == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "        logprint('Database loaded!', logger=logger)\n",
    "    else:\n",
    "        logprint('Database is not available: cannot load the database!', stream='error', logger=logger)\n",
    "        \n",
    "    # shuffle the dataframe\n",
    "    if shuffle and random_state is not None:\n",
    "        logprint('Shuffling database...', logger=logger)\n",
    "        df = skl.utils.shuffle(df, random_state=random_state)\n",
    "        logprint('Database shuffled!', logger=logger)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define some functions we can use to extract and manipulate the database. We use the _Scikit-learn_ API to create _Estimator_ classes (inheriting the _Scikit_ interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# remove the outliers from a Pandas dataset\n",
    "class RemoveOutliers(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Remove outlying data given a dataset and a dictionary containing the intervals for each class.\n",
    "    \n",
    "    E.g.: if the two classes are 'h11' and 'h21', the dictionary will be: {'h11': [1, 16], 'h21': [1, 86]}.\n",
    "    \n",
    "    Public methods:\n",
    "        fit:           unused method\n",
    "        transform:     remove data outside the given interval\n",
    "        fit_transform: equivalent to transform(fit(...))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filter_dict=None):\n",
    "        \"\"\"\n",
    "        Constructor of the class.\n",
    "        \n",
    "        Optional arguments:\n",
    "            filter_dict: the intervals to retain in the data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.filter_dict = filter_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Unused method.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input by deleting data outside the interval\n",
    "        \n",
    "        Required arguments:\n",
    "            X: the dataset\n",
    "            \n",
    "        Returns:\n",
    "            the transformed dataset\n",
    "        \"\"\"\n",
    "\n",
    "        x = X.copy() # avoid overwriting\n",
    "\n",
    "        if self.filter_dict is not None:\n",
    "            for key in self.filter_dict:\n",
    "                x = x.loc[x[key] >= self.filter_dict[key][0]]\n",
    "                x = x.loc[x[key] <= self.filter_dict[key][1]]\n",
    "\n",
    "        return x\n",
    "\n",
    "# extract the tensors from a Pandas dataset\n",
    "class ExtractTensor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extract a dense tensor from sparse input from a given dataset.\n",
    "    \n",
    "    Public methods:\n",
    "        fit:           unused method\n",
    "        transform:     extract dense tensor\n",
    "        fit_transform: equivalent to transform(fit(...))\n",
    "        get_shape:     compute the shape of the tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, flatten=False, shape=None):\n",
    "        \"\"\"\n",
    "        Constructor of the class.\n",
    "        \n",
    "        Optional arguments:\n",
    "            flatten: whether to flatten the output or keep the current shape\n",
    "            shape:   force the computation with a given shape\n",
    "        \"\"\"\n",
    "\n",
    "        self.flatten = flatten\n",
    "        self.shape   = shape\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Unused method.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Compute the dense equivalent of the sparse input.\n",
    "        \n",
    "        Required arguments:\n",
    "            X: the dataset\n",
    "            \n",
    "        Returns:\n",
    "            the transformed input\n",
    "        \"\"\"\n",
    "\n",
    "        x = X.copy() # avoid overwriting\n",
    "        if self.shape is None:\n",
    "            self.shape = x.apply(np.shape).max() # get the shape of the tensor\n",
    "\n",
    "        if len(self.shape) > 0: # apply this to vectors and tensors\n",
    "            offset = lambda s : [ (0, self.shape[i] - np.shape(s)[i]) for i in range(len(self.shape)) ]\n",
    "            x      = x.apply(lambda s: np.pad(s, offset(s), mode='constant'))\n",
    "\n",
    "        if self.flatten and len(self.shape) > 0:\n",
    "            return list(np.stack(x.apply(np.ndarray.flatten).values))\n",
    "        else:\n",
    "            return list(np.stack(x.values))\n",
    "\n",
    "    def get_shape(self):\n",
    "        \"\"\"\n",
    "        Compute the shape of the tensor.\n",
    "        \n",
    "        Returns:\n",
    "            the shape of the tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the define the functions we will use to evaluate and improve the algorithms. Even though the ultimate goal is a regression task, we will however predict integer values $h_{11},~h_{21} \\in \\mathbb{Z}$, thus we will evaluate the **accuracy** of the prediction, given the best estimate (in general we use the _mean squared error_ to evaluate the algorithms, but we accept those with best _accuracy_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy (possibly after rounding)\n",
    "def accuracy_score(y_true, y_pred, rounding=np.rint):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the predictions after rounding.\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values\n",
    "        y_pred: predicted values\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    assert np.shape(y_true)[0] == np.shape(y_pred)[0] # check if same length\n",
    "    \n",
    "    # if same length then proceed\n",
    "    accuracy = 0\n",
    "    if rounding is not None:\n",
    "        for n in range(np.shape(y_true)[0]):\n",
    "            accuracy = accuracy + 1 \\\n",
    "                       if int(y_true[n]) == int(rounding(y_pred[n])) \\\n",
    "                       else accuracy\n",
    "    else:\n",
    "        for n in range(np.shape(y_true)[0]):\n",
    "            accuracy = accuracy + 1 \\\n",
    "                       if y_true[n] == y_pred[n] \\\n",
    "                       else accuracy\n",
    "    return accuracy / np.shape(y_true)[0]\n",
    "\n",
    "# get the error difference (possibly after rounding)\n",
    "def error_diff(y_true, y_pred, rounding=np.rint):\n",
    "    \"\"\"\n",
    "    Compute the error difference between true values and predictions (positive values are overestimate and viceversa).\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values\n",
    "        y_pred: predicted values\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    assert np.shape(y_true)[0] == np.shape(y_pred)[0] # check if same length\n",
    "\n",
    "    # if same length then proceed\n",
    "    err = y_true - rounding(y_pred)\n",
    "    return np.array(err).astype(np.int8)\n",
    "\n",
    "# print *SearchCV scores\n",
    "def gridcv_score(estimator, rounding=np.rint, logger=None):\n",
    "    \"\"\"\n",
    "    Print scores given by cross-validation and optimisation techniques.\n",
    "    \n",
    "    Required arguments:\n",
    "        estimator: the estimator to be evaluated\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "        logger:   the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "    \n",
    "    best_params = estimator.best_params_              # get best parameters\n",
    "    df          = pd.DataFrame(estimator.cv_results_) # dataframe with CV res.\n",
    "    \n",
    "    cv_best_res = df.loc[df['params'] == best_params] # get best results\n",
    "    accuracy    = cv_best_res.loc[:, 'mean_test_score'].values[0]\n",
    "    std         = cv_best_res.loc[:, 'std_test_score'].values[0]\n",
    "    \n",
    "    logprint('Best parameters: {}'.format(best_params), logger=logger)\n",
    "    logprint('Accuracy ({}) of cross-validation: ({:.3f} ± {:.3f})%'.format(rounding.__name__, accuracy*100, std*100), logger=logger)\n",
    "    \n",
    "# print the accuracy of the predictions\n",
    "def prediction_score(estimator, X, y, use_best_estimator=False, rounding=np.rint, logger=None):\n",
    "    \"\"\"\n",
    "    Print the accuracy of the predictions.\n",
    "    \n",
    "    Required arguments:\n",
    "        estimator: the estimator to be used for the predictions\n",
    "        X: the features\n",
    "        y: the labels (actual values)\n",
    "        \n",
    "    Optional arguments:\n",
    "        use_best_estimator: whether to use the estimator.best_estimator_ or just estimator\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "        logger:   the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_best_estimator:\n",
    "        estimator = estimator.best_estimator_\n",
    "    \n",
    "    accuracy = accuracy_score(y, estimator.predict(X), rounding=rounding)\n",
    "    logprint('Accuracy ({}) of the predictions: {:.3f}%'.format(rounding.__name__, accuracy*100), logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use _Matplotlib_ to plot the data and define a few functions which we can use during the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set label sizes\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=12)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# set building block sizes for the plot\n",
    "mpl_width  = 6\n",
    "mpl_height = 5\n",
    "\n",
    "# save the current figure\n",
    "def save_fig(filename, tight_layout=True, extension='png', resolution=96, logger=None):\n",
    "    \"\"\"\n",
    "    Save current figure to file.\n",
    "    \n",
    "    Required arguments:\n",
    "        filename: the name of the file where to save the figure (without extension)\n",
    "        \n",
    "    Optional arguments:\n",
    "        tight_layout: whether to use the tight_layout\n",
    "        extension:    extension of the file to use\n",
    "        resolution:   resolution of the file\n",
    "        logger:       the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "\n",
    "    filename = path.join(IMG_PATH, filename + '.' + extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "\n",
    "    logprint('Saving {}...'.format(filename), logger=logger)\n",
    "    plt.savefig(filename, format=extension, dpi=resolution)\n",
    "    logprint('Saved {}!'.format(filename), logger=logger)\n",
    "\n",
    "# get a generator to count the occurrencies\n",
    "def get_counts(df, label, feature):\n",
    "    \"\"\"\n",
    "    Generator to produce the count of unique occurrencies of the data.\n",
    "    \n",
    "    Required arguments:\n",
    "        df:      the Pandas dataframe\n",
    "        label:   the label to consider\n",
    "        feature: the feature to consider\n",
    "        \n",
    "    Yields:\n",
    "        [ unique feature, unique value, counts ]\n",
    "    \"\"\"\n",
    "\n",
    "    for n in np.sort(df[feature].unique()):\n",
    "        uniques, counts = np.unique(df[label].loc[df[feature] == n].values, return_counts=True)\n",
    "        for u, c in np.c_[uniques, counts]:\n",
    "            yield [ n, u, c ]\n",
    "\n",
    "# plot histogram of occurrencies\n",
    "def count_plot(ax, data, title=None, xlabel=None, ylabel='N',\n",
    "               legend=None, xlog=False, ylog=False, binstep=5,\n",
    "               **kwargs):\n",
    "    \"\"\"\n",
    "    Plot histogram of occurrencies (e.g.: frequency plot).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        binstep:  the distance between adjacent bins\n",
    "        **kwargs: additional arguments to pass to plt.hist\n",
    "    \"\"\"\n",
    "\n",
    "    min_tick = np.min(data) if np.min(data) > -100 else -100 # set a MIN cut\n",
    "    max_tick = np.max(data) if np.max(data) < 100  else 100  # set a MAX cut\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create a grid\n",
    "    ax.set_title(title)                  # set title\n",
    "    ax.set_xlabel(xlabel)                # set a label for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set a label for the y axis\n",
    "    ax.set_xticks(np.arange(min_tick,    # set no. of ticks in the x axis\n",
    "                            max_tick,\n",
    "                            step=binstep\n",
    "                           )\n",
    "                 )\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    ax.hist(data,                        # create histogram using 'step' funct.\n",
    "            histtype='step',\n",
    "            label=legend,\n",
    "            **kwargs)\n",
    "\n",
    "    if legend is not None:               # add legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot labeled features and their values\n",
    "def label_plot(ax, data, title=None, xlabel=None, ylabel='values',\n",
    "               legend=None, xlog=False, ylog=False, binstep=1,\n",
    "               **kwargs):\n",
    "    \"\"\"\n",
    "    Plot values of labelled data (e.g.: variable ranking).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        binstep:  the distance between adjacent bins\n",
    "        **kwargs: additional arguments to pass to plt.plot\n",
    "    \"\"\"\n",
    "\n",
    "    labels      = [f[0] for f in data]   # labels vector\n",
    "    importances = [f[1] for f in data]   # importances vector\n",
    "    length      = len(labels)            # length of the labels vector\n",
    "    \n",
    "    ax.grid(alpha=0.2)                   # create a grid\n",
    "    ax.set_title(title)                  # set title\n",
    "    ax.set_xlabel(xlabel)                # set a label for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set a label for the x axis\n",
    "\n",
    "    ax.set_xticks(np.arange(length,      # set no. of ticks in the x axis\n",
    "                            step=binstep\n",
    "                           )\n",
    "                 )\n",
    "    ax.set_xticklabels(labels,           # set name of labels of the x axis\n",
    "                       ha='right',       # horizontal alignment\n",
    "                       rotation=45       # rotation of the labels\n",
    "                      )\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    ax.plot(np.arange(length),           # plot data\n",
    "            importances,\n",
    "            label=legend,\n",
    "            **kwargs)\n",
    "\n",
    "    if legend is not None:               # add legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot the correlation matrix of a Pandas dataframe\n",
    "def mat_plot(ax, matrix, labels, label='correlation matrix', **kwargs):\n",
    "    \"\"\"\n",
    "    Plot the correlation matrix of a given dataframe.\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:     the subplot ax where to plot data\n",
    "        matrix: the matrix to plot\n",
    "        labels: the labels to show with the matrix\n",
    "        \n",
    "    Optional arguments:\n",
    "        label:    the label to use for the colour bar\n",
    "        **kwargs: additional arguments to pass to plt.matshow\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(labels), # set ticks for x axis\n",
    "                  step=1)\n",
    "                 )\n",
    "    ax.set_xticklabels([''] + labels,    # set the name of the ticks\n",
    "                       rotation=90\n",
    "                      )\n",
    "\n",
    "    ax.set_yticks(np.arange(len(labels), # set ticks for y axis\n",
    "                  step=1)\n",
    "                 )\n",
    "    ax.set_yticklabels([''] + labels)    # set the name of the ticks\n",
    "                    \n",
    "    matshow = ax.matshow(matrix,         # show the matrix\n",
    "                         vmin=-1.0,\n",
    "                         vmax=1.0,\n",
    "                         **kwargs\n",
    "                        )\n",
    "                                \n",
    "    cbar = ax.figure.colorbar(matshow,   # create the colour bar\n",
    "                              ax=ax,\n",
    "                              fraction=0.05,\n",
    "                              pad=0.05\n",
    "                             )\n",
    "    cbar.ax.set_ylabel(label,            # show the colour bar\n",
    "                       va='bottom',      # vertical alignment\n",
    "                       rotation=-90)     # rotation of the label\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot a scatter plot with colours and sizes\n",
    "def scatter_plot(ax, data, title=None, xlabel=None, ylabel=None,\n",
    "                 legend=None, xlog=False, ylog=False,\n",
    "                 colour=True, size=True, colour_label='N', size_leg=0,\n",
    "                 **kwargs):\n",
    "    \"\"\"\n",
    "    Scatter plot of occurrencies with colour and size codes.\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:        the title of the plot\n",
    "        xlabel:       the label of the x axis\n",
    "        ylabel:       the label of the y axis\n",
    "        legend:       the label for the legend in the plot\n",
    "        xlog:         whether to use the log scale on the x axis\n",
    "        ylog:         whether to use the log scale on the y axis\n",
    "        colour:       whether to use colour codes\n",
    "        size:         whether to use entries of different size\n",
    "        colour_label: label to use for the colour code\n",
    "        size_leg:     length of the legend of the size code\n",
    "        **kwargs:     additional arguments to pass to plt.scatter\n",
    "    \"\"\"\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create  a grid\n",
    "    ax.set_xlabel(xlabel)                # set labels for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set labels for the y axis\n",
    "    ax.set_title(title)                  # set title\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    if colour:                           # create the plot with size and colours\n",
    "        if size:\n",
    "            scat = ax.scatter(data[0], data[1], s=data[2], c=data[2], **kwargs)\n",
    "        else:\n",
    "            scat = ax.scatter(data[0], data[1], c=data[2], **kwargs)\n",
    "        cbar = ax.figure.colorbar(scat, ax=ax)\n",
    "        cbar.ax.set_ylabel(colour_label, rotation=-90, va='bottom')\n",
    "    else:\n",
    "        if size:\n",
    "            scat = ax.scatter(data[0], data[1], s=data[2], **kwargs)\n",
    "        else:\n",
    "            scat = ax.scatter(data[0], data[1], **kwargs)\n",
    "\n",
    "    scat.set_label(legend)               # set label of the plot\n",
    "    if size_leg:                         # add the size legend if needed\n",
    "        handles, labels = scat.legend_elements('sizes', num=size_leg)\n",
    "        ax.legend(handles, labels, loc='lower center',\n",
    "                  bbox_to_anchor=(0.5,-0.3), ncol=len(handles),\n",
    "                  fontsize='medium', frameon=False)\n",
    "\n",
    "    if legend:                           # show the legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot a series with trivial x label\n",
    "def series_plot(ax, data, title=None, xlabel='series', ylabel=None,\n",
    "                legend=None, xlog=False, ylog=False,\n",
    "                step=False, std=False,\n",
    "                **kwargs):\n",
    "    \"\"\"\n",
    "    Plot a series of data with ordered x axis (e.g.: epoch series).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        step:     whether to use a step function for the plot\n",
    "        std:      highlight the strip of the standard deviation\n",
    "        **kwargs: additional arguments to pass to plt.step or plot.plot\n",
    "    \"\"\"\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create the grid\n",
    "    ax.set_title(title)                  # set the title\n",
    "    ax.set_xlabel(xlabel)                # set labels for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set labels for the y axis\n",
    "\n",
    "    if xlog:                             # use log scale in the x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in the y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    series = np.arange(len(data))        # create trivial x axis data\n",
    "    if step:                             # create the plot\n",
    "        ax.step(series, data, label=legend, **kwargs)\n",
    "    else:\n",
    "        ax.plot(series, data, label=legend, **kwargs)\n",
    "\n",
    "    if std:                              # show coloured strip with std\n",
    "        ax.fill_between(series,\n",
    "                        data + np.std(data),\n",
    "                        data - np.std(data),\n",
    "                        alpha=0.2)\n",
    "\n",
    "    if legend is not None:               # show the legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further we also set the **memory growth** of the GPU RAM in order to avoid memory issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-09 09:30:33,266 --> ERROR: No GPUs in the setup!\n"
     ]
    }
   ],
   "source": [
    "# get the list of installed GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) # set memory growth\n",
    "            \n",
    "        # get the list of logical devices (GPUs)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        logprint('GPU setup: {:d} physical GPUs, {:d} logical GPUs.'.format(len(gpus), len(logical_gpus)), logger=logger)\n",
    "    except RuntimeError as e:\n",
    "        logprint(e, stream='error', logger=logger)\n",
    "else:\n",
    "    logprint('No GPUs in the setup!', stream='error', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "We then proceed with the analysis by first loading the prepared datasets and then building the appropriate architectures for the prediction of the Hodge numbers. We start from the analysis of the configuration matrix through a **CNN** architecture and then compare the results with the analysis of the engineered dataset.\n",
    "\n",
    "We consider only the configuration matrix and build a CNN to predict the labels. First of all we load the dataset and then divide labels and features, we then proceed with the architecture of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-09 09:40:09,455 --> INFO: Reading database...\n",
      "2020-04-09 09:40:09,512 --> INFO: Database loaded!\n"
     ]
    }
   ],
   "source": [
    "if path.isfile(path.join(ROOT_DIR, DB_NAME + '_matrix.h5')):\n",
    "    matrix = load_dataset(path.join(ROOT_DIR, DB_NAME + '_matrix.h5'), logger=logger)\n",
    "    h11    = ExtractTensor(flatten=False).fit_transform(matrix['h11'])\n",
    "    h21    = ExtractTensor(flatten=False).fit_transform(matrix['h21'])\n",
    "    matrix = ExtractTensor(flatten=False).fit_transform(matrix['matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Preparation of the Input\n",
    "\n",
    "Before splitting the dataset into training and test sets, we need to resize the feature inputs in order to feed them to a convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Reshape the input given as a matrix (rows x cols) into (rows x cols x depth) where depth = 1\n",
    "    \n",
    "    Required arguments:\n",
    "        matrix: the matrix to reshape\n",
    "        \n",
    "    Returns:\n",
    "        the reshaped matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = np.shape(matrix)[0]\n",
    "    cols = np.shape(matrix)[1]\n",
    "    \n",
    "    reshaped_matrix = []\n",
    "    \n",
    "    for m in matrix:\n",
    "        reshaped_matrix.append(np.reshape(matrix, (rows, cols, 1)))\n",
    "        \n",
    "    return reshaped_matrix    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply it to the entire dataset of matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprint('Reshaping the matrix...', logger=logger)\n",
    "matrix = reshape_matrix(matrix)\n",
    "logprint('Matrix reshaped!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the ML analysis, we keep 10% of the whole dataset as **test** set, while we use 90% of it as **training** set. Specifically we then retain 1/9 of the training set as holdout **validation** such that the entire dataset is thus divided into 10% for test predictions, 10% for validation and 80% for effective training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logprint('Splitting into training and test sets...', logger=logger)\n",
    "matrix_train, matrix_test, h11_train, h11_test, h21_train, h21_test = train_test_split(matrix, h11, h21,                   test_size=0.1,     shuffle=False)\n",
    "matrix_train, matrix_val,  h11_train, h11_val,  h21_train, h21_val  = train_test_split(matrix_train, h11_train, h21_train, test_size=1.0/9.0, shuffle=False)\n",
    "logprint('Divided into training and test sets!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "We the build the architecture of the CNN. We use a modular approach in order to be able to change its parameters in a quick way in order to perform the hyperparameter optimization efficiently. We build the network using blocks of **convolutional layers** (`conv_n_blocks`) with several hidden layers inside (`conv_n_hidden_layers`) with a decreasing number of filters in them (`conv_n_initial_filters`) and the **pooling** operation at the end (`conv_pooling`, can be _avg_ for _AvgPool2D_ or _max_ for _MaxPool2D_, with associated `conv_pool_size` for the size of the pooling). The kernel size is decreasing as well and starts at a given size (`conv_kernel_size`) with padding. We then attach the CNN to a **fully connected** (FC) network with blocks of **dense layers** (`dense_n_hidden_layers`) with a decreasing number of neurons in them (`dense_n_initial_neurons`). We also use **batch normalization** (`batch_normalization`, can be true or false) to improve the result and we add **dropout layers** to avoid overfitting (`dropout`, can be _all_ for dropout after every block or _final_ for dropout only before the FC network, and `dropout_rate` for its rate). We also give the possibility to change the activation function for the convolutional and dense layers (`conv_activation`, can be _leaky_ for _LeakyReLU_ or _relu_ for _ReLU_, and `dense_activation`, can be _leaky_ for _LeakyReLU_ or _relu_ for _ReLU_. For the _LeakyReLU_ activation, we also consider `leaky_alpha` for its slope). Since we want to predict $h_{11} \\ge 0$ and $h_{21} \\ge 0$, we also consider whether to use the _ReLU_ activation function in the output layer to force a positive output (`final_relu`, can be true or false). The model creation also involves the learning rate which should be chosen carefully (`learning_rate`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras            import losses, metrics\n",
    "from tensorflow.keras.models     import Model\n",
    "from tensorflow.keras.layers     import Lambda, Conv2D, Dense, Flatten, MaxPool2D, AvgPool2D, Activation, LeakyReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def cnn_model(input_shape,\n",
    "              conv_n_blocks=1,\n",
    "              conv_n_hidden_layers=1,\n",
    "              conv_n_initial_filters=16,\n",
    "              conv_kernel_size=3,\n",
    "              conv_activation='relu',\n",
    "              conv_pooling='max',\n",
    "              conv_pool_size=2,\n",
    "              dense_n_hidden_layers=1,\n",
    "              dense_n_initial_neurons=10,\n",
    "              dense_activation='relu',\n",
    "              leaky_alpha=0.5,\n",
    "              final_relu=False,\n",
    "              batch_normalization=True,\n",
    "              dropout='final',\n",
    "              dropout_rate=0.2,\n",
    "              learning_rate=0.001\n",
    "             ):\n",
    "    \"\"\"\n",
    "    Build a CNN model with blocks of convolutional layers attached to a FC network.\n",
    "    \n",
    "    Required arguments:\n",
    "        input_shape:             the Numpy shape of the input tensor\n",
    "    \n",
    "    Optional arguments:\n",
    "        conv_n_blocks:           the number of convolutional blocks\n",
    "        conv_n_hidden_layers:    the number of convolutional layers in each block\n",
    "        conv_n_initial_filter:   the number of filters in the first convolutional block\n",
    "        conv_kernel_size:        the size of the kernel in the first convolutional block\n",
    "        conv_activation:         the activation function of the convolutional layers\n",
    "        conv_pooling:            the pooling operation at the end of each block\n",
    "        conv_pool_size:          the size of the pooling window\n",
    "        dense_n_hidden_layers:   the number of hidden layers in the FC network\n",
    "        dense_n_initial_neurons: the number of neurons in the first FC layer\n",
    "        dense_activation:        the activation function of the FC network\n",
    "        leaky_alpha:             the slope of the LeakyReLU activation\n",
    "        final_relu:              whether to use ReLU in the output layer\n",
    "        batch_normalization:     whether to use batch normalization\n",
    "        dropout:                 what kind of dropout strategy to use\n",
    "        dropout_rate:            the rate of the dropout\n",
    "        learning_rate:           the learning rate for the gradient descent\n",
    "        \n",
    "    Accepted inputs:\n",
    "        conv_n_blocks:           (int)   strictly positive integer (cannot be 0)\n",
    "        conv_n_hidden_layers:    (int)   strictly positive integer (cannot be 0)\n",
    "        conv_n_initial_filters:  (int)   strictly positive integer (cannot be 0)\n",
    "        conv_kernel_size:        (int)   strictly positive integer (cannot be 0)\n",
    "        conv_activation:         (str)   'relu' for ReLU or 'leaky' for LeakyReLU (see 'leaky_alpha')\n",
    "        conv_pooling:            (str)   'max' for MaxPool, 'avg' for AvgPool or None\n",
    "        conv_pool_size:          (int)   strictly positive integer (must be greater than 1)\n",
    "        dense_n_hidden_layers:   (int)   strictly positive integer (cannot be 0)\n",
    "        dense_n_initial_neurons: (int)   strictly positive integer (cannot be 0)\n",
    "        dense_activation:        (str)   'relu' for ReLU or 'leaky' for LeakyReLU (see 'leaky_alpha')\n",
    "        leaky_alpha:             (float) strictly positive floating point number\n",
    "        final_relu:              (bool)  True or False\n",
    "        batch_normalization:     (bool)  True or False\n",
    "        dropout:                 (str)   'final' for dropout only before the FC network, 'all' for dropout after every block\n",
    "        dropout_rate:            (float) strictly positive floating point number (< 1.0)\n",
    "        learning_rate:           (float) strictly positive floating point number\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute the space of convolutional filters and the kernel sizes, based on the no. of blocks and the no. of initial filters\n",
    "    filters = np.linspace(1, conv_n_initial_filters, conv_n_blocks, dtype=np.int8)\n",
    "    kernels = np.linspace(1, conv_kernel_size,       conv_n_blocks, dtype=np.int8)\n",
    "    \n",
    "    # compute the space of fully connected neurons used in each dense layer, based on the no. of blocks and the no. of initial filters\n",
    "    neurons = np.linspace(2, dense_n_initial_neurons, dense_n_hidden_layers, dtype=np.int8)\n",
    "    \n",
    "    # build the convolutional model\n",
    "    i = Input(shape=input_shape) #---------------------------------------------------------- INPUT layer\n",
    "    x = Lambda(lambda x: x)(i)\n",
    "    \n",
    "    for n_block in range(conv_n_blocks): #-------------------------------------------------- START convolutional block\n",
    "        \n",
    "        x = BatchNormalization()(x) if batch_normalization #------------------------------------ ADD batch normalization\n",
    "        \n",
    "        for n_layer in range(conv_n_hidden_layers): #------------------------------------------- START hidden convolutional layers\n",
    "            \n",
    "            x = Conv2D(filters=filters[conv_n_blocks - 1 - n_block],\n",
    "                       kernel_size=kernels[conv_n_blocks - 1 - n_block],\n",
    "                       padding='same'\n",
    "                      ) #--------------------------------------------------------------------------- ADD convolutional layer\n",
    "            \n",
    "            x = Activation('relu') if   conv_activation == 'relu' \\\n",
    "                                   else LeakyReLU(alpha=leaky_alpha) #------------------------------ ADD activation function\n",
    "            \n",
    "            #----------------------------------------------------------------------------------- END hidden convolutional layers\n",
    "            \n",
    "        if conv_pooling is not None and \\\n",
    "        (x.output_shape[0] > conv_pool_size and x.output_shape[1] > conv_pool_size):\n",
    "            \n",
    "            x = MaxPool2D(pool_size=conv_pool_size, \\\n",
    "                          padding='same') if conv_pooling == 'max' \\\n",
    "                                          else AvgPool2D(pool_size=conv_pool_size, \\\n",
    "                                                         padding='same') #---------------------- ADD pooling (if allowed and requested)\n",
    "            \n",
    "        if dropout == 'all':\n",
    "            x = Dropout(rate=dropout_rate)(x) #------------------------------------------------- ADD dropout (if requested)\n",
    "            \n",
    "        #----------------------------------------------------------------------------------- END convolutional block\n",
    "        \n",
    "    x = Flatten()(x) #---------------------------------------------------------------------- START fully connected network\n",
    "\n",
    "    x = Dropout(rate=dropout_rate)(x) #----------------------------------------------------- ADD dropout\n",
    "    \n",
    "    for n_layer in range(dense_n_hidden_layers): #------------------------------------------ START hidden dense layers\n",
    "        \n",
    "        x = BatchNormalization()(x) if batch_normalization #------------------------------------ ADD batch normalization\n",
    "        \n",
    "        x = Dense(units=neurons[dense_n_hidden_layers - 1 - n_layer])(x) #---------------------- ADD dense layer\n",
    "        \n",
    "        x = Activation('relu') if dense_activation == 'relu' \\\n",
    "                               else LeakyReLU(alpha=leaky_alpha) #------------------------------ ADD activation layer\n",
    "        \n",
    "        if dropout == 'all':\n",
    "            x = Dropout(rate=dropout_rate)(x) #------------------------------------------------- ADD dropout (if requested)\n",
    "            \n",
    "        #----------------------------------------------------------------------------------- END hidden dense layers\n",
    "        \n",
    "    f = Dense(1)(x) #----------------------------------------------------------------------- OUTPUT layer\n",
    "    \n",
    "    f = Activation('relu') if final_relu #-------------------------------------------------- OUTPUT layer (w/ activation)\n",
    "    \n",
    "    # create the model and compile it\n",
    "    model = Model(inputs=i, outputs=f)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate, loss=losses.MeanSquaredError, metrics=[metrics.MeanSquaredError]))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then consider the function to maximize, namely the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from skopt                       import gp_minimize\n",
    "from skopt.plots                 import plot_convergence\n",
    "from skopt.space                 import Categorical, Integer, Real\n",
    "from skopt.utils                 import use_named_args\n",
    "from tensorflow.keras.callbacks  import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils      import plot_model\n",
    "\n",
    "# decide the input shape\n",
    "input_shape = np.shape(matrix[0])\n",
    "\n",
    "# decide the number of samplings to perform on the search space\n",
    "n_iter = 10\n",
    "\n",
    "# create search space\n",
    "search_params = [Integer(1,  5,                      prior='uniform',     name='conv_n_blocks'),\n",
    "                 Integer(1,  5,                      prior='uniform',     name='conv_n_hidden_layers'),\n",
    "                 Integer(16, 512,                    prior='uniform',     name='conv_n_initial_filters'),\n",
    "                 Integer(2,  6,                      prior='uniform',     name='conv_kernel_size'),\n",
    "                 Integer(2,  4,                      prior='uniform',     name='conv_pool_size'),\n",
    "                 Integer(0,  10,                     prior='uniform',     name='dense_n_hidden_layers'),\n",
    "                 Integer(1e0, 1e3,          base=10, prior='log-uniform', name='dense_n_initial_neurons'),\n",
    "                 Real(0.1, 0.5,                      prior='uniform',     name='dropout_rate'),\n",
    "                 Real(1.0e-2, 1.0e1,        base=10, prior='log-uniform', name='leaky_alpha'),\n",
    "                 Real(1.0e-3, 1.0e-1,       base=10, prior='log-uniform', name='learning_rate'),\n",
    "                 Integer(False, True,                                     name='final_relu'),\n",
    "                 Integer(False, True,                                     name='batch_normalization'),\n",
    "                 Categorical(['relu', 'leaky'],                           name='conv_activation'),\n",
    "                 Categorical(['relu', 'leaky'],                           name='dense_activation'),\n",
    "                 Categorical(['max', 'avg', None],                        name='conv_pooling'),\n",
    "                 Categorical(['all', 'final'],                            name='dropout')\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit the model for the first Hodge number, namely $h_{11}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(search_params)\n",
    "def objective_h11(**params):\n",
    "    \n",
    "    # create model\n",
    "    logprint('Building a CNN model...', logger=logger)\n",
    "    model = cnn_model(input_shape=input_shape, **params)\n",
    "    \n",
    "    # print summary\n",
    "    model.summary()\n",
    "    \n",
    "    # fit the model (for the moment no checkpoint save needed: once we have the best parameters, then we train and save the best)\n",
    "    logprint('Fitting a CNN model...', logger=logger)\n",
    "    model.fit(x=matrix_train,\n",
    "              y=h11_train,\n",
    "              batch_size=32,\n",
    "              epochs=500,\n",
    "              verbose=1,\n",
    "              callbacks=EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True),\n",
    "              validation_data=(matrix_val, h11_val)\n",
    "             )\n",
    "    \n",
    "    # evaluate the model on the validation set and return accuracy (actually 1 - acc to be minimized)\n",
    "    logprint('Evaluating model...', logger=logger)\n",
    "    predictions = model.predict(matrix_val)\n",
    "    return 1.0 - accuracy_score(predictions, h11_val, rounding=np.rint)\n",
    "\n",
    "# compute the minimization\n",
    "model_h11_res = gp_minimize(objective_h11, search_params, n_calls=n_iter, random_state=RAND)\n",
    "\n",
    "# return the best parameters and print them\n",
    "best_params = {search_params[n].name: model_h11_res.x[n] for n in range(len(model_h11_res.x))}\n",
    "model_h11   = cnn_model(input_shape=input_shape, **best_params)\n",
    "model_h11_json = model_h11.to_json()\n",
    "with open(path.join(MOD_PATH, 'cnn_matrix_h11_arch.json'), 'w') as f:\n",
    "    json.dump(model_h11_json, f)\n",
    "\n",
    "# fit the best model on the training set and compute the accuracy of the predictions\n",
    "logprint('Best parameters: {}'.format(best_params), logger=logger)\n",
    "model_h11.fit(x=matrix_train,\n",
    "              y=h11_train,\n",
    "              batch_size=32,\n",
    "              epochs=500,\n",
    "              verbose=1,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True),\n",
    "                         ModelCheckpoint(filepath=path.join(MOD_PATH, 'cnn_matrix_h11_weights.h5'), monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "                        ],\n",
    "              validation_data=(matrix_val, h11_val)\n",
    "             )\n",
    "\n",
    "preds_score_h11 = prediction_score(estimator=model_h11, X=matrix_test, y=h11_test, use_best_estimator=False, rounding=np.rint, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then repeat the same procedure for the second Hodge number, $h_{21}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(search_params)\n",
    "def objective_h21(**params):\n",
    "    \n",
    "    # create model\n",
    "    logprint('Building a CNN model...', logger=logger)\n",
    "    model = cnn_model(input_shape=input_shape, **params)\n",
    "    \n",
    "    # print summary\n",
    "    model.summary()\n",
    "    \n",
    "    # fit the model (for the moment no checkpoint save needed: once we have the best parameters, then we train and save the best)\n",
    "    logprint('Fitting a CNN model...', logger=logger)\n",
    "    model.fit(x=matrix_train,\n",
    "              y=h21_train,\n",
    "              batch_size=32,\n",
    "              epochs=500,\n",
    "              verbose=1,\n",
    "              callbacks=EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True),\n",
    "              validation_data=(matrix_val, h21_val)\n",
    "             )\n",
    "    \n",
    "    # evaluate the model on the validation set and return accuracy (actually 1 - acc to be minimized)\n",
    "    logprint('Evaluating model...', logger=logger)\n",
    "    predictions = model.predict(matrix_val)\n",
    "    return 1.0 - accuracy_score(predictions, h21_val, rounding=np.rint)\n",
    "\n",
    "# compute the minimization\n",
    "model_h21_res = gp_minimize(objective_h21, search_params, n_calls=n_iter, random_state=RAND)\n",
    "\n",
    "# return the best parameters and print them\n",
    "best_params = {search_params[n].name: model_h21_res.x[n] for n in range(len(model_h21_res.x))}\n",
    "model_h21   = cnn_model(input_shape=input_shape, **best_params)\n",
    "model_h21_json = model_h21.to_json()\n",
    "with open(path.join(MOD_PATH, 'cnn_matrix_h21_arch.json'), 'w') as f:\n",
    "    json.dump(model_h21_json, f)\n",
    "\n",
    "# fit the best model on the training set and compute the accuracy of the predictions\n",
    "logprint('Best parameters: {}'.format(best_params), logger=logger)\n",
    "model_h21.fit(x=matrix_train,\n",
    "              y=h21_train,\n",
    "              batch_size=32,\n",
    "              epochs=500,\n",
    "              verbose=1,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True),\n",
    "                         ModelCheckpoint(filepath=path.join(MOD_PATH, 'cnn_matrix_h21_weights.h5'), monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "                        ],\n",
    "              validation_data=(matrix_val, h21_val)\n",
    "             )\n",
    "\n",
    "preds_score_h21 = prediction_score(estimator=model_h21, X=matrix_test, y=h21_test, use_best_estimator=False, rounding=np.rint, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
