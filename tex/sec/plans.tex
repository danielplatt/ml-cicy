One thing that may be worth trying is to decrease the tolerance of the linear and support vector models and increase the maximum number of iterations to improve the accuracy results.

We should the include the results of artificial neural networks and ensemble learning (stacking)\footnote{I decided to go one step back and implement a better architecture for the networks, thus I only have some preliminary results using the old architecture. For instance with a better (and quite longer analysis) I was able to improve also the \texttt{Sequential} models and reach 88\% of accuracy for $h_{11}$ and 36\% for $h_{21}$.}: even though they still have to be better implemented, the \texttt{Sequential} model in \textit{Keras} can reach accuracy above 84\% for $h_{11}$ but still below 40\% for $h_{21}$ while its \texttt{Functional} API counterpart (using \texttt{Conv2D}) slightly improves the results but not dramatically. The \texttt{Conv1D} and \texttt{Dense} models lead to much worse results\footnote{This may be due to the fact that \texttt{Conv1D} operates on the input comparing entries trying to extract features with the kernel function, while the \texttt{PCA} has essentially broken the possibility to find related entries in the matrix by mixing them. The same applies to the \texttt{Dense} models which treats all entries as separate. Given the way neural networks work, it might as well be that the right choice is to use the matrix directly.}. Using the stacking learning (naively with all previous algorithms stacked together\footnote{It  might be a good idea to drop the \texttt{Conv1D} and \texttt{Dense} models on the \texttt{PCA} components, since their results are visibly worse on the entire range of $h_{11}$ and $h_{21}$: I do not expect them to help much since the other algorithms produce better predictions in the entire range.}) it is however possible to improve the results reaching almost 90\% for $h_{11}$ and 43\% for $h_{21}$.