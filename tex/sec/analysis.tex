\subsection{Training and Test Sets and Validation Strategy}
    In the analysis we first split the entire dataset into a training and a test subsets. We retain 10\% of the data as test while we take 90\% of the dataset as training set. We also implement cross-validation for the evaluation of the algorithms (not the neural networks): we use a \texttt{KFold} approach with 9 splits, which amount to using 80\% of the total dataset as training set while retaining another 10\% as validation set for each of the 9 folds. For the neural networks we will simply use holdout validation with 10\% of the total data as validation set and 80\% as training.
    
\subsection{Metrics and Evaluation}
    While we are interested in a regression task, the final results are however related to the prediction of integer numbers, as $h_{11} \in \mathds{R}$ and $h_{21} \in \mathds{R}$, typical of a classification task. We choose to implement a custom metric using the \texttt{make\_scorer} present in the \textit{Scikit} library and which can be used inside \texttt{GridSearchCV} and similar hyperparameter optimization algorithms: we use the accuracy of the fitted estimator after rounding the result. In general we consider the \textit{floor} of the predictions, exception made for \texttt{SVR} and the neural networks which use the rounding to next integer\footnote{Actually also the second level of the stacked learner uses rounding to next integer.}.

\subsection{Machine Learning Analysis}
    For the analysis we consider the following algorithms:
    \begin{itemize}
        \item \textit{Scikit-learn} implementations:
            \subitem \texttt{LinearRegression},
            \subitem \texttt{Lasso},
            \subitem \texttt{ElasticNet},
            \subitem \texttt{Ridge},
            \subitem \texttt{LinearSVR},
            \subitem \texttt{SVR} (with \textit{rbf} kernel);
        \item \textit{XGBoost} implementations:
            \subitem \texttt{XGBRegressor} (implementing boosted decision trees),
            \subitem \texttt{XGBRFRegressor} (implementing random forests of decision trees).
    \end{itemize}
    The reason behind the choice of the algorithms is related to the previous data visualisation and pre-analysis. In particular we study the correlation of the data using several linear models implementing different types of regression: specifically we are interested in \texttt{Lasso} and \texttt{Ridge} models as they implement $l1$ and $l2$ regularisation, respectively, while \texttt{ElasticNet} implements both. At the same time, given the geometrical disposition of the data it may be worth to try a linear approach to \textit{Support Vectors} and a Gaussian kernel. The choice of \textit{XGBoost} for the decision trees is related to speed and simplicity in the implementation which can be easily moved to the GPU for evaluation (even though in this particular case, all computations have been performed on CPU for parallelization purposes): they implement histogram-based methods for the trees which recently proved to be faster and reliable \cite{ke2017lightgbm}.

    For each algorithm we first compute a baseline using just the components of the configuration matrix, we then compute two intermediate results using \texttt{num\_cp} only and \texttt{dim\_cp} only (i.e. the scalar and the vector variables with highest rankings). We then compute the algorithm with the engineered dataset and compare the improvement to the previous results. We implement Bayesan search of the best hyperparameter using the \texttt{BayesSearchCV} algorithms provided by the \textit{Scikit-optimize} library\footnote{Since the \texttt{LinearRegression} has only two usable hyperparameters (namely \texttt{fit\_intercept} and \texttt{normalize}), we used the usual \texttt{GridSearchCV} only in this case.}: while \texttt{GridSearchCV} would have been impractical for most algorithms, the alternative \texttt{RandomizedSearchCV} does not take advantage of the search space. In this particular analysis the Bayesan approach provided by \texttt{BayesSearchCV} significantly helps especially in fine tuning the hyperparameter space of algorithms such as the support vectors and the decision trees which undergo large variations due to different hyperparameter choices\footnote{Using \texttt{RandomizedSearchCV} usually leads to slightly worse or equal results in the algorithms.}. The best algorithms is then chosen based on the cross-validation results (we use 50 iterations of the \texttt{BayesSearchCV} for almost all the algorithms exception made for the decision trees which undertake only 15 iterations due to the amount of RAM needed to load the dataset for each iteration).

\subsection{Neural Networks}
    We then implement the following neural networks (NN) architectures:
    \begin{itemize}
        \item \texttt{Sequential} model using only the configuration matrix,
        \item \texttt{Functional} model using the configuration matrix and the engineered features (not the \texttt{PCA} results, though),
        \item \texttt{Functional} model using the components of the \texttt{PCA} of the matrix (as a vector to be used with \texttt{Conv1D} layers) in place of the matrix itself and the engineered features,
        \item \texttt{Functional} model using the components of the \texttt{PCA} of the matrix (as separate scalar features to be used with only \texttt{Dense} layers) in place of the matrix itself and the engineered features.
    \end{itemize}
    In this case we do not use any automatic optimisation for the hyperparameters mainly due to hardware and time restrictions\footnote{It is also a good practice to use the \textit{Keras} backend (add \texttt{from tensorflow.keras import backend as K} at the beginning of any script) and add \texttt{K.clear\_session()} to clear GPU memory after every training.}. However we use holdout validation as a testing ground for the architectures. The main differences between the architectures will be their implementations: in the first two cases we will use \texttt{Conv2D} layers for the matrix components in order to apply kernel transformation and construct possible patterns; in the third case we use \texttt{Conv1D} layers to treat the \texttt{PCA} as a vector, while in the fourth case we only use \texttt{Dense} layers in the architecture.
    
    During compilation we use the \textit{Adam} optimizer and the mean squared error as metric\footnote{It may be worth trying to use a custom metric implementing the accuracy after rounding. However the \textit{Tensorflow} backend uses a system of placeholder tensors which have to be figured out prior to their implementation.}. We also use a variable learning rate using the \texttt{ReduceLROnPlateau} callback in \textit{Keras} to reduce it by a factor 0.3 when it stalls for more than 30 epochs.
    
    The hyperparameter space to be tuned is made of several different possibilities including the number of layers and connections for each input, the kernel size of the convolution layers (\texttt{padding = ``same''}), possible \texttt{MaxPool} layers and their size, dropout and batch normalizations layers, activation functions (for hidden layers whether \textit{ReLU} or \textit{LeakyReLU} and whether to use \textit{ReLU} for the output), \textit{l1} and \textit{l2} regularization for the kernel (i.e. \texttt{kernel\_regularization}). 
    
\subsection{Ensemble Learning}
    Eventually we also implement stacked learning using the previously mentioned algorithms (apart from \textit{XGBoost}'s \texttt{XGBRegressor} substituted by \textit{Scikit}'s \texttt{GradientBoostingRegressor} and \texttt{XGBRFRegressor} substituted by \texttt{RandomForestRegressor}\footnote{As of March 6th, it seems that the experimental \textit{Scikit} API implemented by \textit{XGBoost} keeps loading the dataset for every iteration of the process quickly filling the RAM of the system which starts to swap and ultimately stops. Since the algorithm behind the computation is the same but the implementation slightly differs between \textit{XGBoost} (histogram based) and \textit{Scikit} (traditional approach) there may be a couple of solutions. The first is to use the \textit{Learning} API provided by \textit{XGBoost} and implement the \texttt{BayesSearchCV} from scratch using \textit{Scikit-optimize} low-level API (at the moment of writing, this may be quite long to do and prone to error). The second would be to switch directly to the \textit{Scikit} implementation which seems to be more reliable towards RAM consumption even though we did not implemented it in the first part of the analysis: apparently, the results are comparable to the \textit{XGBoost} library (in terms of accuracy, certainly not in computational speed). To be noted, \textit{Scikit} does not use second order approximation in the computation of the gradient descent: this might lead to a bit of improvement in the accuracy of the \texttt{GradientBoostingRegressor}.}) and neural network architectures. In this case we keep the same test set, while we split the training set in two: we will use 60\% of the total set for the first level training and 30\% for prediction and second level training. The last 10\% will be the test set. We then train the algorithms on the first fold as we did in the previous part of the analysis and we use the algorithms to make predictions on the second set which we use to train several meta learner to be tested against the test set. In particular we compare:
    \begin{itemize}
        \item \texttt{LinearRegression},
        \item \texttt{SVR} (with \textit{rbf} kernel),
        \item \texttt{RandomForestRegressor}.
    \end{itemize}
    In order to properly fit the algorithms, both in the first and in the second level training, we use again \texttt{BayesSearchCV} as hyperparameter optimization: we sample each parameter space 50 times as in the first part of the analysis, exception made for the \texttt{RandomForestRegressor} and the \texttt{GradientBoostingRegressor} which will undergo only \texttt{50 / 5 = 10} iterations. We use a cross-validation strategy with 6 splits for the first level predictions (thus using 10\% of the total dataset as validation and 50\% for training for each fold) and 3 splits for the second level predictions.