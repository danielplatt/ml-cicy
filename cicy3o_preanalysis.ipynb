{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Complete Intersection Calabi-Yau Manifolds\n",
    "\n",
    "We use a machine learning (ML) approach predict the **Hodge numbers** $h_{11}$ and $h_{21}$ of **Calabi-Yau** (CY) 3-folds in the framework of **String Theory**. First of all we analyse the dataset of the manifolds and build a consistent preanalysis to visualise the distribution of the data and extract the necessary features. We will first plot different distributions of the data and performed unsupervised tasks to find structures in them (in particular we use **clustering** and **principal components analysis**). We then study the **variable ranking** of the features and try to figure out the \"good\" features we can use in the real analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure\n",
    "\n",
    "We print information about the current OS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current OS:                  Linux (kernel release: 5.6.10-arch1-1, architecture: x86_64)\n",
      "Number of available threads: 8\n",
      "Current CPU frequency:       3538 MHz (max: 3800 MHz)\n",
      "Available RAM memory:        11006 MB (tot: 15758 MB)\n"
     ]
    }
   ],
   "source": [
    "from mltools.libos import InfoOS\n",
    "\n",
    "print('Current OS:                  {} (kernel release: {}, architecture: {})'.format(InfoOS().os, InfoOS().kernel, InfoOS().arch))\n",
    "print('Number of available threads: {:d}'.format(InfoOS().threads))\n",
    "print('Current CPU frequency:       {:.0f} MHz (max: {:.0f} MHz)'.format(InfoOS().freq, InfoOS().freqm))\n",
    "print('Available RAM memory:        {:d} MB (tot: {:d} MB)'.format(InfoOS().vmav, InfoOS().vmtot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future use, we establish early in the notebook the number of maximum jobs that every algorithm can take concurrently. Thus, if we want to run parallel notebooks with different jobs, we will not encounter issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = int(InfoOS().threads / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print information on the current GPU setup (if available):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  6 17:36:22 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce 940MX       Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8    N/A /  N/A |      5MiB /  2004MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     58639      G   /usr/lib/Xorg                                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We import the Python modules we use and print their versions to keep track of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7e59a37494f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m      \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m     \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m       \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# We still need all the names that are toplevel on tensorflow_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# These should not be visible in the main tf module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/site-packages/tensorflow_core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/site-packages/tensorflow_core/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/site-packages/tensorflow_core/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0m_current_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m   _current_module.__path__ = (\n\u001b[1;32m    666\u001b[0m       [_module_util.get_parent_dir(estimator)] + _current_module.__path__)\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0m_names_with_underscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_s\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFModuleWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ml/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import random     as rnd\n",
    "import sklearn    as skl\n",
    "import skopt      as sko\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow       import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning) # ignore user warnings: nothing that I can really do anything about it...\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=12)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# print the version of the modules\n",
    "print('Python version: {:d}.{:d}'      .format(sys.version_info.major, sys.version_info.minor))\n",
    "print('Matplot version: {}'            .format(mpl.__version__))\n",
    "print('Numpy version: {}'              .format(np.__version__))\n",
    "print('Pandas version: {}'             .format(pd.__version__))\n",
    "print('Scikit-learn version: {}'       .format(skl.__version__))\n",
    "print('Scikit-optimize version: {}'    .format(sko.__version__))\n",
    "print('Tensorflow version: {}'         .format(tf.__version__))\n",
    "print('Keras version: {} (backend: {})'.format(keras.__version__, K.backend()))\n",
    "\n",
    "# fix random_seed\n",
    "RAND = 42\n",
    "rnd.seed(RAND)\n",
    "np.random.seed(RAND)\n",
    "tf.random.set_seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Preparation\n",
    "\n",
    "In order to save the results of the analysis, we define where to store images, log files and models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "\n",
    "ROOT_DIR = '.' #-------------------------------------------------- root directory\n",
    "IMG_DIR  = 'img' #------------------------------------------------ directory of images\n",
    "MOD_DIR  = 'models' #--------------------------------------------- directory of saved models\n",
    "LOG_DIR  = 'log' #------------------------------------------------ directory of logs\n",
    "\n",
    "DB_NAME = 'cicy3o' #---------------------------------------------- name of the dataset\n",
    "DB_FILE = DB_NAME + '.h5' #--------------------------------------- full name with extension\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE) #-------------------------- full path of the dataset\n",
    "DB_DIR  = 'original' if DB_NAME == 'cicy3o' else 'favourable' #--- subdir where to store images, models, logs\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR, DB_DIR)\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR, DB_DIR)\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR, DB_DIR)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok=True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok=True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a log file to store debug and related information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from mltools.liblog import create_logfile\n",
    "\n",
    "path_to_log = path.join(LOG_PATH,\n",
    "                        DB_NAME + '_preanalysis.log'\n",
    "                       )\n",
    "log = create_logfile(path_to_log,\n",
    "                     name=DB_NAME + '_preanalysis',\n",
    "                     level=logging.DEBUG\n",
    "                    )\n",
    "\n",
    "# these lines provide the same setup also for the Jupyter logging\n",
    "logger = logging.getLogger() #------------------------------------------------- get the current logging session\n",
    "\n",
    "fmt = logging.Formatter('%(asctime)s: %(levelname)s ==> %(message)s') #-------- customise the formatting options\n",
    "\n",
    "handler = logging.StreamHandler() #-------------------------------------------- handle the stream to the default (stderr)\n",
    "handler.setLevel(logging.DEBUG) #---------------------------------------------- print everything\n",
    "handler.setFormatter(fmt) #---------------------------------------------------- set the formatting options\n",
    "\n",
    "logger.handlers = [handler] #-------------------------------------------------- override the default stream\n",
    "\n",
    "# we are ready to go!\n",
    "log.info('New logging session started. Log is at {}.'.format(path_to_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally set the _memory growth_ property of the GPU in order to avoid overflowing its RAM memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU') #--------------------------------------- list of physical GPUs\n",
    "\n",
    "if gpus: #----------------------------------------------------------------------------------------- set memory growth only if GPU is active\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) #---------------------------------- set memory growth\n",
    "            \n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU') #------------------------ list of logical devices\n",
    "        print('GPU setup: {:d} physical GPUs, {:d} logical GPUs.'.format(len(gpus),\n",
    "                                                                         len(logical_gpus)\n",
    "                                                                        )\n",
    "             )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPUs in the setup!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Dataset\n",
    "\n",
    "We then fetch the dataset from its location at http://www.lpthe.jussieu.fr/~erbin/files/data/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "from os     import remove\n",
    "from urllib import request as rq\n",
    "\n",
    "URL_ROOT = 'http://www.lpthe.jussieu.fr/~erbin/files/data/' #------------------------- root of the URL\n",
    "TAR_FILE = DB_NAME + '_data.tar.gz' #------------------------------------------------- name of the tarball located at the URL\n",
    "TAR_PATH = path.join(ROOT_DIR, TAR_FILE) #-------------------------------------------- path of the tarball in the local system\n",
    "\n",
    "def extract_tar(tarname, dbfile, root):\n",
    "    '''\n",
    "    Extract database from tarball in the root directory.\n",
    "    \n",
    "    Required arguments:\n",
    "        tarname: the tarball file,\n",
    "        dbfile:  the name of the file to extract,\n",
    "        root:    where to extract the file.\n",
    "    '''\n",
    "    with tarfile.open(tarname, 'r') as tar:\n",
    "        tar.extract(dbfile, path=root)\n",
    "    \n",
    "if not path.isfile(DB_PATH): #-------------------------------------------------------- check if the dataset is already present locally\n",
    "    if not path.isfile(TAR_PATH): #--------------------------------------------------- check if the tarball is already present locally\n",
    "        _, message = rq.urlretrieve(URL_ROOT + TAR_FILE, TAR_PATH) #------------------ download the file\n",
    "        log.debug('Remote file saved in {}.'.format(TAR_PATH))\n",
    "        \n",
    "        extract_tar(TAR_PATH, DB_FILE, ROOT_DIR) #------------------------------------ extract the dataset\n",
    "        remove(TAR_PATH) #------------------------------------------------------------ remove tarball\n",
    "        log.debug('Dataset saved in {}'.format(DB_PATH))\n",
    "    else:\n",
    "        extract_tar(TAR_PATH, DB_FILE, ROOT_DIR) #------------------------------------ extract the dataset\n",
    "        remove(TAR_PATH) #------------------------------------------------------------ remove tarball\n",
    "        log.debug('Dataset saved in {}'.format(DB_PATH))\n",
    "else:\n",
    "    log.debug('Dataset already dowloaded in {}.'.format(DB_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preanalysis\n",
    "\n",
    "We use [Scikit-learn](https://scikit-learn.org/stable/index.html) as **Python** library for the preanalysis and [Matplotlib](https://matplotlib.org/) to plot the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "We first load and select the entries of the dataset which we use during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "if path.isfile(DB_PATH):\n",
    "    df = pd.read_hdf(DB_PATH)\n",
    "    log.debug('Database loaded.')\n",
    "    log.info('Shape is {:d} rows x {:d} columns.'.format(df.shape[0], df.shape[1]))\n",
    "else:\n",
    "    log.error('Cannot load database from {}!'.format(DB_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print a few statistics of the current dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the `dtypes` of the columns since they will be relevant for the subsequent analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then focus only on manifolds which **are not** direct products of spaces, that is we keep only `isprod = 0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only isprod = 0 and remove the corresponding column\n",
    "df_noprod = df.loc[df['isprod'] == 0].drop(columns=['isprod'])\n",
    "log.debug('Database reshaped to {:d} rows x {:d} columns.'.format(df_noprod.shape[0], df_noprod.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualise the distribution of the labels $h_{11}$ and $h_{21}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools.libplot import Plot\n",
    "\n",
    "plot = Plot(rows=2, columns=3)\n",
    "\n",
    "plot.hist2D(df['h11'],        axis=(0,0), title='Distribution of $h_{11}$',                    xlabel='$h_{11}$', ylabel='#', ylog=True)\n",
    "plot.hist2D(df_noprod['h11'], axis=(0,1), title='Distribution of $h_{11}$ w/o Product Spaces', xlabel='$h_{11}$', ylabel='#', ylog=True)\n",
    "plot.hist2D(df_noprod['h11'], axis=(0,2), title='Comparison of the Distributions of $h_{11}$', xlabel='$h_{11}$', ylabel='#', ylog=True, legend='no prod.')\n",
    "plot.hist2D(df['h11'],        axis=(0,2), title='Comparison of the Distributions of $h_{11}$', xlabel='$h_{11}$', ylabel='#', ylog=True, legend='original')\n",
    "\n",
    "plot.hist2D(df['h21'],        axis=(1,0), title='Distribution of $h_{21}$',                    xlabel='$h_{21}$', ylabel='#', ylog=True, binstep=10)\n",
    "plot.hist2D(df_noprod['h21'], axis=(1,1), title='Distribution of $h_{21}$ w/o Product Spaces', xlabel='$h_{21}$', ylabel='#', ylog=True, binstep=10)\n",
    "plot.hist2D(df_noprod['h21'], axis=(1,2), title='Comparison of the Distributions of $h_{21}$', xlabel='$h_{21}$', ylabel='#', ylog=True, legend='no prod.', binstep=10)\n",
    "plot.hist2D(df['h21'],        axis=(1,2), title='Comparison of the Distributions of $h_{21}$', xlabel='$h_{21}$', ylabel='#', ylog=True, legend='original', binstep=10)\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'labels_distribution'))\n",
    "log.debug('Plot saved to {}.'.format(path.join(IMG_PATH, 'labels_distribution.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the Outliers\n",
    "\n",
    "In order to improve the training of the algorithms, we then remove the outliers of the distributions. We keep $h_{11} \\in \\left[ 1, 16 \\right]$ and $h_{21} \\in \\left[ 1, 86 \\right]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools.libtransformer import RemoveOutliers\n",
    "\n",
    "filter_dict = {'h11': [1, 16],\n",
    "               'h21': [1, 86]\n",
    "              }\n",
    "\n",
    "df_noprod_noout = RemoveOutliers(filter_dict=filter_dict).fit_transform(df_noprod)\n",
    "log.debug('Database reshaped to {:d} rows x {:d} columns.'.format(df_noprod_noout.shape[0], df_noprod_noout.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then show the distribution of the newly extracted dataset in comparison with the previous versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Plot(rows=2, columns=2)\n",
    "\n",
    "plot.hist2D(df_noprod_noout['h11'], axis=(0,0), title='Distribution of $h_{11}$ w/o Product Spaces and Outliers', xlabel='$h_{11}$', ylabel='#', ylog=True)\n",
    "plot.hist2D(df_noprod_noout['h11'], axis=(0,1), title='Comparison of the Distributions of $h_{11}$',              xlabel='$h_{11}$', ylabel='#', ylog=True, legend='no prod. + no out.')\n",
    "plot.hist2D(df['h11'],              axis=(0,1), title='Comparison of the Distributions of $h_{11}$',              xlabel='$h_{11}$', ylabel='#', ylog=True, legend='original')\n",
    "\n",
    "plot.hist2D(df_noprod_noout['h21'], axis=(1,0), title='Distribution of $h_{21}$ w/o Product Spaces and Outliers', xlabel='$h_{21}$', ylabel='#', ylog=True, binstep=5)\n",
    "plot.hist2D(df_noprod_noout['h21'], axis=(1,1), title='Comparison of the Distributions of $h_{21}$',              xlabel='$h_{21}$', ylabel='#', ylog=True, legend='no prod. + no out.', binstep=5)\n",
    "plot.hist2D(df['h21'],              axis=(1,1), title='Comparison of the Distributions of $h_{21}$',              xlabel='$h_{21}$', ylabel='#', ylog=True, legend='original', binstep=5)\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'labels_distribution_noout'))\n",
    "log.debug('Plot saved to {}.'.format(path.join(IMG_PATH, 'labels_distribution_noouts.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation\n",
    "\n",
    "We can then move to the visualisation of the distribution of the labels as a function of several features (mainly scalars for visualisation reasons). In particular we first consider the scatter plot of $h_{11}$ and $h_{21}$ as functions of the **scalar** features and the **correlation matrix** of them, before moving to computing the **principal component analysis** (PCA) in 2D to plot the configuration matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(df, label, feature):\n",
    "    '''\n",
    "    Generator to produce the count of unique occurrencies of the data.\n",
    "    \n",
    "    Required arguments:\n",
    "        df:      the Pandas dataframe\n",
    "        label:   the label to consider\n",
    "        feature: the feature to consider\n",
    "        \n",
    "    Yields:\n",
    "        np.array([ unique feature, unique value, counts ])\n",
    "    '''\n",
    "\n",
    "    for n in np.sort(df[feature].unique()):\n",
    "        uniques, counts = np.unique(df[label].loc[df[feature] == n].values,\n",
    "                                    return_counts=True)\n",
    "        \n",
    "        for u, c in np.c_[uniques, counts]:\n",
    "            yield np.array([ n, u, c ])\n",
    "\n",
    "plot = Plot(rows=5, columns=2)\n",
    "\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h11', 'num_cp'))).T,\n",
    "               axis=(0,0),\n",
    "               title='Distribution of $h_{11}$ vs num_cp',\n",
    "               xlabel='num_cp',\n",
    "               ylabel='$h_{11}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h11', 'num_eqs'))).T,\n",
    "               axis=(1,0),\n",
    "               title='Distribution of $h_{11}$ vs num_eqs',\n",
    "               xlabel='num_eqs',\n",
    "               ylabel='$h_{11}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h11', 'norm_matrix'))).T,\n",
    "               axis=(2,0),\n",
    "               title='Distribution of $h_{11}$ vs norm_matrix',\n",
    "               xlabel='norm_matrix',\n",
    "               ylabel='$h_{11}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h11', 'rank_matrix'))).T,\n",
    "               axis=(3,0),\n",
    "               title='Distribution of $h_{11}$ vs rank_matrix',\n",
    "               xlabel='rank_matrix',\n",
    "               ylabel='$h_{11}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h11', 'num_ex'))).T,\n",
    "               axis=(4,0),\n",
    "               title='Distribution of $h_{11}$ vs num_ex',\n",
    "               xlabel='num_ex',\n",
    "               ylabel='$h_{11}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h21', 'num_cp'))).T,\n",
    "               \n",
    "               axis=(0,1),\n",
    "               title='Distribution of $h_{21}$ vs num_cp',\n",
    "               xlabel='num_cp',\n",
    "               ylabel='$h_{21}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h21', 'num_eqs'))).T,\n",
    "               axis=(1,1),\n",
    "               title='Distribution of $h_{21}$ vs num_eqs',\n",
    "               xlabel='num_eqs',\n",
    "               ylabel='$h_{21}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h21', 'norm_matrix'))).T,\n",
    "               axis=(2,1),\n",
    "               title='Distribution of $h_{21}$ vs norm_matrix',\n",
    "               xlabel='norm_matrix',\n",
    "               ylabel='$h_{21}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h21', 'rank_matrix'))).T,\n",
    "               axis=(3,1),\n",
    "               title='Distribution of $h_{21}$ vs rank_matrix',\n",
    "               xlabel='rank_matrix',\n",
    "               ylabel='$h_{21}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=np.array(list(get_counts(df, 'h21', 'num_ex'))).T,\n",
    "               axis=(4,1),\n",
    "               title='Distribution of $h_{21}$ vs num_ex',\n",
    "               xlabel='num_ex',\n",
    "               ylabel='$h_{21}$',\n",
    "               size_labels=5,\n",
    "               alpha=0.65\n",
    "              )\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'scalar_features_distribution'))\n",
    "log.debug('Plot saved to {}.'.format(path.join(IMG_PATH, 'scalar_features_distribution.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "\n",
    "We then compute the correlation matrix of the scalar features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_noprod_noout[['h11', 'h21', 'num_cp', 'num_eqs', 'num_cp_1', 'num_cp_2', 'num_cp_neq1', 'num_over', 'num_ex', 'rank_matrix', 'norm_matrix']].corr()\n",
    "\n",
    "Plot().matrix(data=corr, title='Correlation Matrix', xticks=corr.columns, yticks=corr.columns).save_and_close(path.join(IMG_PATH, 'correlation_matrix'))\n",
    "log.debug('Plot saved to {}.'.format(path.join(IMG_PATH, 'correlation_matrix.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Samples\n",
    "\n",
    "We then shuffle the database to avoid inserting ordering bias in clustering and PCA. In principle there tons of ways to do it, but we choose the _Pandas_ way: using the function `sample` we return 100% of the database (`frac=1`) in random order (use `random_state` for consistency between runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = df_noprod_noout.sample(frac=1, random_state=RAND) #---- get 100% of the dataset in random order\n",
    "log.debug('Dataframe shuffled with random state {:d}.'.format(RAND))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means Clustering\n",
    "\n",
    "We then compute the **K Means** clustering labels for the matrix components to find structure in the data and be able to improve the training algorithms. This is an **unsupervised** preanalyisis: in this particular case we do not need to divide the dataset into training and validation sets, however it is best to shuffle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltools.libtransformer import ExtractTensor\n",
    "\n",
    "# flatten the matrix to use in clustering and \n",
    "matrix = np.array(ExtractTensor(flatten=True).fit_transform(df_shuffled['matrix']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we proceed with the clustering analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusters      = range(2,16) #------------------------------------------------ study clusters in range [2, 15]\n",
    "kmeans_labels = np.zeros((np.shape(matrix)[0], np.shape(clusters)[0])) #----- shape: n_samples x n_clusters (store the labels)\n",
    "\n",
    "for n in clusters:\n",
    "    kmeans_labels[:,n-clusters.start] = KMeans(n_clusters=n,\n",
    "                                               n_jobs=n_jobs,\n",
    "                                               random_state=RAND\n",
    "                                              ).fit_predict(matrix)\n",
    "    log.info('Computed K Means with {:d} clusters.'.format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the clustering labels can be visualised in a scatter plot. We pick some values of the number of clusters and show the ripartition of the labels using a different colour code for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Plot(rows=2, columns=2)\n",
    "\n",
    "plot.scatter2D(data=[df_shuffled['h11'].values, df_shuffled['h21'].values, kmeans_labels[:,0]],\n",
    "               axis=(0,0),\n",
    "               title='K Means Distribution with 2 clusters',\n",
    "               xlabel='$h_{11}$',\n",
    "               ylabel='$h_{21}$',\n",
    "               size=False,\n",
    "               colour=True,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=[df_shuffled['h11'].values, df_shuffled['h21'].values, kmeans_labels[:,4]],\n",
    "               axis=(0,1),\n",
    "               title='K Means Distribution with 6 clusters',\n",
    "               xlabel='$h_{11}$',\n",
    "               ylabel='$h_{21}$',\n",
    "               size=False,\n",
    "               colour=True,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=[df_shuffled['h11'].values, df_shuffled['h21'].values, kmeans_labels[:,8]],\n",
    "               axis=(1,0),\n",
    "               title='K Means Distribution with 10 clusters',\n",
    "               xlabel='$h_{11}$',\n",
    "               ylabel='$h_{21}$',\n",
    "               size=False,\n",
    "               colour=True,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=[df_shuffled['h11'].values, df_shuffled['h21'].values, kmeans_labels[:,13]],\n",
    "               axis=(1,1),\n",
    "               title='K Means Distribution with 15 clusters',\n",
    "               xlabel='$h_{11}$',\n",
    "               ylabel='$h_{21}$',\n",
    "               size=False,\n",
    "               colour=True,\n",
    "               alpha=0.65\n",
    "              )\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'cluster_distribution'))\n",
    "log.debug('Plot saved to {}.'.format(path.join(IMG_PATH, 'cluster_distribution.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "We then proceed with the **PCA** analysis with a twofold purpose: we first compute the 2 principal components of the **configuration matrix** to be able to plot the distribution of the matrix entries in a 2D plot, we then compute the PCA with **99% of the variance** retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca2D_transformer = PCA(n_components=2, random_state=RAND)\n",
    "pca2D = pca2D_transformer.fit_transform(matrix)\n",
    "log.info('PCA variance retained for each component: {}'.format(pca2D_transformer.explained_variance_ratio_))\n",
    "\n",
    "pca2D_plot_h11 = np.array([pca2D[:,0], pca2D[:,1], df_shuffled['h11'].values])\n",
    "pca2D_plot_h21 = np.array([pca2D[:,0], pca2D[:,1], df_shuffled['h21'].values])\n",
    "\n",
    "plot = Plot(rows=1, columns=2)\n",
    "\n",
    "plot.scatter2D(data=pca2D_plot_h11,\n",
    "               axis=0,\n",
    "               title='PCA Distribution of $h_{11}$',\n",
    "               xlabel='principal component no. 1',\n",
    "               ylabel='principal component no. 2',\n",
    "               size=False,\n",
    "               alpha=0.65\n",
    "              )\n",
    "plot.scatter2D(data=pca2D_plot_h21,\n",
    "               axis=1,\n",
    "               title='PCA Distribution of $h_{21}$',\n",
    "               xlabel='principal component no. 1',\n",
    "               ylabel='principal component no. 2',\n",
    "               size=False,\n",
    "               alpha=0.65\n",
    "              )\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'pca_distribution'))\n",
    "log.debug('Plot saved to {}.'.format(path.join(IMG_PATH, 'pca_distribution.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the PCA with **99% of variance** retained (this is the PCA which will end up in the final analysis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca99 = PCA(n_components=0.99, random_state=RAND).fit_transform(matrix)\n",
    "log.info('PCA principal components: {:d}'.format(np.shape(pca99)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Ranking\n",
    "\n",
    "We now select the features we are interested in ranking in order to select only those which can actually contribute to the prediction of $h_{11}$ and $h_{21}$. We divide them into **scalar**, **vector** and **tensor** (matrix) features and extract them from the dataset (we do not use them all: we avoid columns with _mean_, _max_, _min_ (_et similia_) in the name since they can be easily extracted from other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = ['num_cp', 'num_eqs', 'num_cp_1', 'num_cp_2', 'num_cp_neq1', 'num_over', 'num_ex', 'rank_matrix', 'norm_matrix']\n",
    "vectors = ['dim_cp', 'num_dim_cp', 'deg_eqs', 'num_deg_eqs', 'dim_h0_amb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build a matrix to hold each entry of the dataset in rows and each feature in columns. We start from the scalars and then we add the vectors after the extraction from the sparse format in which they are currently stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_features = np.zeros((np.shape(df_shuffled)[0], np.shape(scalars)[0]), dtype=np.float) #---- shape: n_samples x n_scalar_features\n",
    "ranking_lengths  = np.ones(np.shape(scalars)[0], dtype=np.int) #----------------------------------- shape: 1 x n_scalar_features\n",
    "\n",
    "for n in range(np.shape(scalars)[0]):\n",
    "    ranking_features[:,n] = ExtractTensor(flatten=True).fit_transform(df_shuffled[scalars[n]]) #--- put features in columns (samples in rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the vector features from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in vectors:\n",
    "    vector           = ExtractTensor(flatten=True).fit_transform(df_shuffled[feature]) #---- extract the feature\n",
    "    ranking_features = np.c_[ranking_features, vector] #------------------------------------ concatenate (put another column to ranking_features)\n",
    "    ranking_lengths  = np.append(ranking_lengths, np.shape(vector)[1]) #-------------------- append length to vector of lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to the variable ranking in two ways:\n",
    "\n",
    "1. using `ranking_features`, `kmeans_labels` and `matrix`\n",
    "2. using `ranking_features`, `kmeans_labels` and `pca99`\n",
    "\n",
    "to avoid redundancies in the computation. We use a **Random Forest** algorithm in the _Scikit_ implementation. At this stage we are not interested in predictions, thus it is not necessary to fine tune the algorithm. Being the random forest and ensemble estimator, we use a reasonable amount of base estimators and do not worry too much about overfitting the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mltools.libscore import Score\n",
    "\n",
    "rnd_for = RandomForestRegressor(n_estimators=100, # 100 trees in the forest is reasonably fast to pull off\n",
    "                                random_state=RAND,\n",
    "                                n_jobs=n_jobs\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start from the first variable ranking (with the configuration matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.c_[ranking_features,\n",
    "                 kmeans_labels,\n",
    "                 matrix] #------------------------------------------------------------------------- build the feature vector\n",
    "\n",
    "#############################\n",
    "#                           #\n",
    "# LABEL LIST                #\n",
    "#                           #\n",
    "#############################\n",
    "ranking_labels = [] #------------------------------------------------------------------------------ initialise label list\n",
    "for n in range(np.shape(scalars + vectors)[0]):\n",
    "    ranking_labels.append((scalars + vectors)[n]) #------------------------------------------------ add feature label\n",
    "    for _ in range(ranking_lengths[n]-1):\n",
    "        ranking_labels.append('') #---------------------------------------------------------------- add empty separator for each entry after the first\n",
    "    \n",
    "for n in range(np.shape(kmeans_labels)[1]):\n",
    "    ranking_labels.append('{:d} clusters'.format(n + clusters.start)) #---------------------------- add clustering labels\n",
    "    \n",
    "ranking_labels.append('matrix') #------------------------------------------------------------------ add matrix label\n",
    "for _ in range(np.shape(matrix)[1]-1):\n",
    "    ranking_labels.append('') #-------------------------------------------------------------------- add empty separator for each entry after the first\n",
    "\n",
    "#############################\n",
    "#                           #\n",
    "# RANDOM FOREST FIT         #\n",
    "#                           #\n",
    "#############################\n",
    "rnd_for.fit(X=features,\n",
    "            y=df_shuffled['h11'].values,\n",
    "           ) #------------------------------------------------------------------------------------- fit the random forest on h11\n",
    "score = Score(y_true=df_shuffled['h11'].values,\n",
    "              y_pred=rnd_for.predict(features),\n",
    "              rounding=np.floor\n",
    "             ) #----------------------------------------------------------------------------------- get training score for debug purposes\n",
    "log.info('Accuracy of ranking (with matrix) on h11: {:.3f}%'.format(score.accuracy()*100)) #------- get training accuracy\n",
    "\n",
    "importances_mat_h11 = list(zip(ranking_labels, rnd_for.feature_importances_)) #-------------------- get the importances for h11\n",
    "\n",
    "rnd_for.fit(X=features,\n",
    "            y=df_shuffled['h21'].values,\n",
    "           ) #------------------------------------------------------------------------------------- fit the random forest on h21\n",
    "score = Score(y_true=df_shuffled['h21'].values,\n",
    "              y_pred=rnd_for.predict(features),\n",
    "              rounding=np.floor\n",
    "             ) #----------------------------------------------------------------------------------- get training score for debug purposes\n",
    "log.info('Accuracy of ranking (with matrix) on h21: {:.3f}%'.format(score.accuracy()*100)) #------- get training accuracy\n",
    "\n",
    "importances_mat_h21 = list(zip(ranking_labels, rnd_for.feature_importances_)) #-------------------- get the importances for h21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the same with the PCA components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.c_[ranking_features,\n",
    "                 kmeans_labels,\n",
    "                 pca99] #-------------------------------------------------------------------------- build the feature vector\n",
    "\n",
    "#############################\n",
    "#                           #\n",
    "# LABEL LIST                #\n",
    "#                           #\n",
    "#############################\n",
    "ranking_labels = [] #------------------------------------------------------------------------------ initialise label list\n",
    "for n in range(np.shape(scalars + vectors)[0]):\n",
    "    ranking_labels.append((scalars + vectors)[n]) #------------------------------------------------ add feature label\n",
    "    for _ in range(ranking_lengths[n]-1):\n",
    "        ranking_labels.append('') #---------------------------------------------------------------- add empty separator for each entry after the first\n",
    "    \n",
    "for n in range(np.shape(kmeans_labels)[1]):\n",
    "    ranking_labels.append('{:d} clusters'.format(n + clusters.start)) #---------------------------- add clustering labels\n",
    "    \n",
    "ranking_labels.append('pca') #--------------------------------------------------------------------- add matrix label\n",
    "for _ in range(np.shape(pca99)[1]-1):\n",
    "    ranking_labels.append('') #-------------------------------------------------------------------- add empty separator for each entry after the first\n",
    "\n",
    "#############################\n",
    "#                           #\n",
    "# RANDOM FOREST FIT         #\n",
    "#                           #\n",
    "#############################\n",
    "rnd_for.fit(X=features,\n",
    "            y=df_shuffled['h11'].values,\n",
    "           ) #------------------------------------------------------------------------------------- fit the random forest on h11\n",
    "score = Score(y_true=df_shuffled['h11'].values,\n",
    "              y_pred=rnd_for.predict(features),\n",
    "              rounding=np.floor\n",
    "             ) #----------------------------------------------------------------------------------- get training score for debug purposes\n",
    "log.info('Accuracy of ranking (with PCA) on h11: {:.3f}%'.format(score.accuracy()*100)) #---------- get training accuracy\n",
    "\n",
    "importances_pca_h11 = list(zip(ranking_labels, rnd_for.feature_importances_)) #-------------------- get the importances for h11\n",
    "\n",
    "rnd_for.fit(X=features,\n",
    "            y=df_shuffled['h21'].values,\n",
    "           ) #------------------------------------------------------------------------------------- fit the random forest on h21\n",
    "score = Score(y_true=df_shuffled['h21'].values,\n",
    "              y_pred=rnd_for.predict(features),\n",
    "              rounding=np.floor\n",
    "             ) #----------------------------------------------------------------------------------- get training score for debug purposes\n",
    "log.info('Accuracy of ranking (with PCA) on h21: {:.3f}%'.format(score.accuracy()*100)) #---------- get training accuracy\n",
    "\n",
    "importances_pca_h21 = list(zip(ranking_labels, rnd_for.feature_importances_)) #-------------------- get the importances for h21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate Feature Visualisation\n",
    "\n",
    "We then proceed with the visualisation of the feature importances. We first consider the various features separately: we plot each scalar, vector and tensor feature with the importance of each of their components. We also show the difference between the variable ranking with the matrix and the PCA by plotting the results in two side-by-side columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data for the plots\n",
    "scalars_mat_lab  = [i[0] for i in importances_mat_h11[0:np.shape(scalars)[0]]]\n",
    "scalars_mat_h11  = [i[1] for i in importances_mat_h11[0:np.shape(scalars)[0]]]\n",
    "scalars_mat_h21  = [i[1] for i in importances_mat_h21[0:np.shape(scalars)[0]]] #------------------------- scalar features with the matrix\n",
    "\n",
    "scalars_pca_lab  = [i[0] for i in importances_pca_h11[0:np.shape(scalars)[0]]]\n",
    "scalars_pca_h11  = [i[1] for i in importances_pca_h11[0:np.shape(scalars)[0]]]\n",
    "scalars_pca_h21  = [i[1] for i in importances_pca_h21[0:np.shape(scalars)[0]]] #------------------------- scalar features with the pca\n",
    "\n",
    "vector_index     = np.shape(scalars)[0] + np.sum(ranking_lengths[np.shape(scalars)[0]:])\n",
    "vectors_mat_lab  = [i[0] for i in importances_mat_h11[np.shape(scalars)[0]:vector_index]]\n",
    "vectors_mat_h11  = [i[1] for i in importances_mat_h11[np.shape(scalars)[0]:vector_index]]\n",
    "vectors_mat_h21  = [i[1] for i in importances_mat_h21[np.shape(scalars)[0]:vector_index]] #-------------- vector features with the matrix\n",
    "\n",
    "vectors_pca_lab  = [i[0] for i in importances_pca_h11[np.shape(scalars)[0]:vector_index]]\n",
    "vectors_pca_h11  = [i[1] for i in importances_pca_h11[np.shape(scalars)[0]:vector_index]]\n",
    "vectors_pca_h21  = [i[1] for i in importances_pca_h21[np.shape(scalars)[0]:vector_index]] #-------------- vector features with the pca\n",
    "\n",
    "cluster_index    = vector_index + np.shape(kmeans_labels)[1]\n",
    "clusters_mat_lab = [i[0] for i in importances_mat_h11[vector_index:cluster_index]]\n",
    "clusters_mat_h11 = [i[1] for i in importances_mat_h11[vector_index:cluster_index]]\n",
    "clusters_mat_h21 = [i[1] for i in importances_mat_h21[vector_index:cluster_index]] #--------------------- cluster ranking with the matrix\n",
    "\n",
    "clusters_pca_lab = [i[0] for i in importances_pca_h11[vector_index:cluster_index]]\n",
    "clusters_pca_h11 = [i[1] for i in importances_pca_h11[vector_index:cluster_index]]\n",
    "clusters_pca_h21 = [i[1] for i in importances_pca_h21[vector_index:cluster_index]] #--------------------- cluster ranking with the pca\n",
    "\n",
    "mat_lab          = [i[0] for i in importances_mat_h11[cluster_index:]]\n",
    "mat_h11          = [i[1] for i in importances_mat_h11[cluster_index:]]\n",
    "mat_h21          = [i[1] for i in importances_mat_h21[cluster_index:]] #--------------------------------- matrix importances\n",
    "\n",
    "pca_lab          = [i[0] for i in importances_pca_h11[cluster_index:]]\n",
    "pca_h11          = [i[1] for i in importances_pca_h11[cluster_index:]]\n",
    "pca_h21          = [i[1] for i in importances_pca_h21[cluster_index:]] #--------------------------------- pca importances\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plot = Plot(rows=4, columns=2)\n",
    "\n",
    "plot.series2D(data=scalars_mat_h11,  axis=(0,0), title='Scalar Features Importances with the Matrix', ylabel='importance', legend='$h_{11}$', labels=scalars_mat_lab)\n",
    "plot.series2D(data=scalars_mat_h21,  axis=(0,0), title='Scalar Features Importances with the Matrix', ylabel='importance', legend='$h_{21}$', labels=scalars_mat_lab)\n",
    "plot.series2D(data=scalars_pca_h11,  axis=(0,1), title='Scalar Features Importances with the PCA',    ylabel='importance', legend='$h_{11}$', labels=scalars_pca_lab)\n",
    "plot.series2D(data=scalars_pca_h21,  axis=(0,1), title='Scalar Features Importances with the PCA',    ylabel='importance', legend='$h_{21}$', labels=scalars_pca_lab)\n",
    "\n",
    "plot.series2D(data=vectors_mat_h11,  axis=(1,0), title='Vector Features Importances with the Matrix', ylabel='importance', legend='$h_{11}$', labels=vectors_mat_lab)\n",
    "plot.series2D(data=vectors_mat_h21,  axis=(1,0), title='Vector Features Importances with the Matrix', ylabel='importance', legend='$h_{21}$', labels=vectors_mat_lab)\n",
    "plot.series2D(data=vectors_pca_h11,  axis=(1,1), title='Vector Features Importances with the PCA',    ylabel='importance', legend='$h_{11}$', labels=vectors_pca_lab)\n",
    "plot.series2D(data=vectors_pca_h21,  axis=(1,1), title='Vector Features Importances with the PCA',    ylabel='importance', legend='$h_{21}$', labels=vectors_pca_lab)\n",
    "\n",
    "plot.series2D(data=clusters_mat_h11, axis=(2,0), title='Clustering Importances with the Matrix',      ylabel='importance', legend='$h_{11}$', labels=clusters_mat_lab)\n",
    "plot.series2D(data=clusters_mat_h21, axis=(2,0), title='Clustering Importances with the Matrix',      ylabel='importance', legend='$h_{21}$', labels=clusters_mat_lab)\n",
    "plot.series2D(data=clusters_pca_h11, axis=(2,1), title='Clustering Importances with the PCA',         ylabel='importance', legend='$h_{11}$', labels=clusters_pca_lab)\n",
    "plot.series2D(data=clusters_pca_h21, axis=(2,1), title='Clustering Importances with the PCA',         ylabel='importance', legend='$h_{21}$', labels=clusters_pca_lab)\n",
    "\n",
    "plot.series2D(data=mat_h11,          axis=(3,0), title='Importances of the Components of the Matrix', ylabel='importance', legend='$h_{11}$', labels=mat_lab, binstep=10)\n",
    "plot.series2D(data=mat_h21,          axis=(3,0), title='Importances of the Components of the Matrix', ylabel='importance', legend='$h_{21}$', labels=mat_lab, binstep=10)\n",
    "plot.series2D(data=pca_h11,          axis=(3,1), title='Importances of the Components of the PCA',    ylabel='importance', legend='$h_{11}$', labels=pca_lab, binstep=10)\n",
    "plot.series2D(data=pca_h21,          axis=(3,1), title='Importances of the Components of the PCA',    ylabel='importance', legend='$h_{21}$', labels=pca_lab, binstep=10)\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'feature_importances'))\n",
    "log.debug('Plot saved to {}.'.format(path.join(IMG_PATH, 'feature_importances.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Feature Visualisation\n",
    "\n",
    "To better visualise the results we also compute the sum of the importances to compare the various contributions: we compute the sum of components of each vector and tensor feature and compare, then we compute the sum of all scalar, vector, tensor features and compare.\n",
    "\n",
    "We can then plot separately the cumulative contributions of the vector and tensor features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of the scalar contributions\n",
    "scalars_sum_mat_h11  = np.sum([i[1] for i in importances_mat_h11[0:np.shape(scalars)[0]]])\n",
    "scalars_sum_mat_h21  = np.sum([i[1] for i in importances_mat_h21[0:np.shape(scalars)[0]]])\n",
    "scalars_sum_pca_h11  = np.sum([i[1] for i in importances_pca_h11[0:np.shape(scalars)[0]]])\n",
    "scalars_sum_pca_h21  = np.sum([i[1] for i in importances_pca_h21[0:np.shape(scalars)[0]]])\n",
    "\n",
    "# sum of the vector contributions (cumulative)\n",
    "vectors_sum_mat_h11  = np.sum([i[1] for i in importances_mat_h11[np.shape(scalars)[0]:vector_index]])\n",
    "vectors_sum_mat_h21  = np.sum([i[1] for i in importances_mat_h21[np.shape(scalars)[0]:vector_index]])\n",
    "vectors_sum_pca_h11  = np.sum([i[1] for i in importances_pca_h11[np.shape(scalars)[0]:vector_index]])\n",
    "vectors_sum_pca_h21  = np.sum([i[1] for i in importances_pca_h21[np.shape(scalars)[0]:vector_index]])\n",
    "\n",
    "# sum of vectors contributions (separate)\n",
    "vectors_sum_separate_mat_h11 = []\n",
    "vectors_sum_separate_mat_h21 = []\n",
    "vectors_sum_separate_pca_h11 = []\n",
    "vectors_sum_separate_pca_h21 = []\n",
    "\n",
    "i = 0\n",
    "for l in ranking_lengths[np.shape(scalars)[0]:]:\n",
    "    vectors_sum_separate_mat_h11.append(np.sum(vectors_mat_h11[i:i+l]))\n",
    "    vectors_sum_separate_mat_h21.append(np.sum(vectors_mat_h21[i:i+l]))\n",
    "    vectors_sum_separate_pca_h11.append(np.sum(vectors_pca_h11[i:i+l]))\n",
    "    vectors_sum_separate_pca_h21.append(np.sum(vectors_pca_h21[i:i+l]))\n",
    "    i = i + l\n",
    "\n",
    "# sum of the cluster contributions\n",
    "clusters_sum_mat_h11 = np.sum([i[1] for i in importances_mat_h11[vector_index:cluster_index]])\n",
    "clusters_sum_mat_h21 = np.sum([i[1] for i in importances_mat_h21[vector_index:cluster_index]])\n",
    "clusters_sum_pca_h11 = np.sum([i[1] for i in importances_pca_h11[vector_index:cluster_index]])\n",
    "clusters_sum_pca_h21 = np.sum([i[1] for i in importances_pca_h21[vector_index:cluster_index]])\n",
    "\n",
    "# sum of the matrix and pca contribution\n",
    "mat_sum_h11          = np.sum([i[1] for i in importances_mat_h11[cluster_index:]])\n",
    "mat_sum_h21          = np.sum([i[1] for i in importances_mat_h21[cluster_index:]])\n",
    "pca_sum_h11          = np.sum([i[1] for i in importances_pca_h11[cluster_index:]])\n",
    "pca_sum_h21          = np.sum([i[1] for i in importances_pca_h21[cluster_index:]])\n",
    "\n",
    "plot = Plot(rows=2, columns=2)\n",
    "\n",
    "plot.series2D(data=[scalars_sum_mat_h11, vectors_sum_mat_h11, clusters_sum_mat_h11, mat_sum_h11],\n",
    "              axis=(0,0),\n",
    "              title='Sum of the Contributions with the Matrix',\n",
    "              ylabel='importance',\n",
    "              legend='$h_{11}$',\n",
    "              labels=['scalars', 'vectors', 'clustering', 'matrix']\n",
    "             )\n",
    "plot.series2D(data=[scalars_sum_mat_h21, vectors_sum_mat_h21, clusters_sum_mat_h21, mat_sum_h21],\n",
    "              axis=(0,0),\n",
    "              title='Sum of the Contributions with the Matrix',\n",
    "              ylabel='importance',\n",
    "              legend='$h_{21}$',\n",
    "              labels=['scalars', 'vectors', 'clustering', 'matrix']\n",
    "             )\n",
    "\n",
    "plot.series2D(data=[scalars_sum_pca_h11, vectors_sum_pca_h11, clusters_sum_pca_h11, pca_sum_h11],\n",
    "              axis=(0,1),\n",
    "              title='Sum of the Contributions with the PCA',\n",
    "              ylabel='importance',\n",
    "              legend='$h_{11}$',\n",
    "              labels=['scalars', 'vectors', 'clustering', 'pca']\n",
    "             )\n",
    "plot.series2D(data=[scalars_sum_pca_h21, vectors_sum_pca_h21, clusters_sum_pca_h21, pca_sum_h21],\n",
    "              axis=(0,1),\n",
    "              title='Sum of the Contributions with the PCA',\n",
    "              ylabel='importance',\n",
    "              legend='$h_{21}$',\n",
    "              labels=['scalars', 'vectors', 'clustering', 'pca']\n",
    "             )\n",
    "\n",
    "plot.series2D(data=vectors_sum_separate_mat_h11,\n",
    "              axis=(1,0),\n",
    "              title='Sum of Vector Contributions with the Matrix',\n",
    "              ylabel='importance',\n",
    "              legend='$h_{11}$',\n",
    "              labels=vectors\n",
    "             )\n",
    "plot.series2D(data=vectors_sum_separate_mat_h21,\n",
    "              axis=(1,0),\n",
    "              title='Sum of Vector Contributions with the Matrix',\n",
    "              ylabel='importance',\n",
    "              legend='$h_{21}$',\n",
    "              labels=vectors\n",
    "             )\n",
    "\n",
    "plot.series2D(data=vectors_sum_separate_pca_h11,\n",
    "              axis=(1,1),\n",
    "              title='Sum of Vector Contributions with the Matrix',\n",
    "              ylabel='importance',\n",
    "              legend='$h_{11}$',\n",
    "              labels=vectors\n",
    "             )\n",
    "plot.series2D(data=vectors_sum_separate_pca_h21,\n",
    "              axis=(1,1),\n",
    "              title='Sum of Vector Contributions with the Matrix',\n",
    "              ylabel='importance',\n",
    "              legend='$h_{21}$',\n",
    "              labels=vectors\n",
    "             )\n",
    "\n",
    "plot.save_and_close(path.join(IMG_PATH, 'feature_importances_sum'))\n",
    "log.debug('Plot saved to {}.'.format(path.join(IMG_PATH, 'feature_importances_sum.pdf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Feature Selection\n",
    "\n",
    "From the results of the preanalysis we see that there are indeed features which play a stronger role in predicting the labels. In particular we find that **more correlated** features (see the [correlation matrix](#Correlation-Matrix)) are usually **more relevant** in a decision tree, at least for scalar features. For instance `num_cp` (and `norm_matrix`, even though more marginally and mostly for $h_{11}$ more than $h_{21}$) is a relevant **scalar** feature (see the [feature importances](#Separate-Feature-Visualisation)) for the predictions of both $h_{11}$ and $h_{21}$ and we should certainly select it in the analysis.\n",
    "\n",
    "The vector features are instead a bit more complicated and dependending on the use of the **configuration matrix** or its **pca**, we get slightly different results: in both cases `dim_cp` is a good marker for the predictions of both Hodge numbers (and we will certainly select it), while `dim_h0_amb` is extremely relevant only for $h_{21}$ (as shown in the [cumulative plots](#Cumulative-Feature_Visualisation)). For $h_{11}$ it gains a bit more relevance when the **configuration matrix** is used.\n",
    "\n",
    "The [clustering](#K-Means-Clustering) analysis shows that there are no visible or recognizable clusters in the data. In fact even the variable ranking seems to hint to the fact that more clusters usually lead to more relevant features (the final result with 15 clusters for $h_{11}$ is almost equal to a classification task, since the number of clusters almost equals the number of \"classes\" in the label $h_{11}$, that is the number of clusters spans the entire range of $h_{11}$). Additionally, even choosing the more relevant number of clusters, the _K Means_ analysis does not play a relevant role and should not be considered as a feature for the analysis.\n",
    "\n",
    "Finally the [PCA](#PCA) certainly contains more relevant information than the matrix itself and it will end up being the final choice, even though we will perform a baseline analysis containing only the matrix for comparison.\n",
    "\n",
    "In order to avoid introducing personal bias or arbitrariness, we then choose only the features with a variable ranking $\\ge 10%$ (for the vector and tensor features this is computed cumulatively, not per entry). We will therefore select the following features, which will then be addressed collectively as the _feature engineered_ set:\n",
    "\n",
    "- `num_cp`, `dim_cp`, `pca99` for the prediction of $h_{11}$,\n",
    "- `num_cp`, `dim_cp`, `dim_h0_amb`, `pca99` for the prediction of $h_{21}$.\n",
    "\n",
    "We will then perform the following analysis:\n",
    "\n",
    "- a baseline using `matrix` for both $h_{11}$ and $h_{21}$ (both for performance analysis and comparison with [_Bull et al._](https://arxiv.org/abs/1806.03121)),\n",
    "- a second baseline using the most important feature for both $h_{11}$ and $h_{21}$ (in this case `num_cp`),\n",
    "- a check using only the engineered features such as `num_cp`, `dim_cp` (and `dim_h0_amb` for $h_{21}$) without `matrix` or `pca99`,\n",
    "- the complete result using the feature engineered set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using the features to be extracted\n",
    "pd.options.mode.chained_assignment = None #-------------------------------------------------------- allow assignement as in Python dictionaries\n",
    "df_extracted        = df_shuffled[['h11', 'h21', 'num_cp', 'dim_cp', 'dim_h0_amb', 'matrix']] #---- select features and labels\n",
    "df_extracted['pca'] = pca99.tolist() #------------------------------------------------------------- add pca column (needs .tolist() to be inserted)\n",
    "\n",
    "# save new database to file\n",
    "df_extracted.to_hdf(path.join(ROOT_DIR, DB_NAME + '_analysis.h5'),\n",
    "                    key=DB_NAME,\n",
    "                    mode='w',\n",
    "                    complevel=9,\n",
    "                    complib='bzip2'\n",
    "                   )\n",
    "log.debug('New database saved to {}.'.format(path.join(ROOT_DIR, DB_NAME + '_analysis.h5')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
