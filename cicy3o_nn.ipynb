{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks for Complete Intersection Calabi-Yau Manifold\n",
    "\n",
    "In the framework of String Theory, we apply **deep learning** techniques for the prediction of the **Hodge numbers** of _Complete Intersection Calabi-Yau_ (CICY) 3-folds. The relevant quantities are therefore $h_{11}$ and $h_{21}$ which can be predicted starting from the configuration matrices of known manifolds.\n",
    "\n",
    "We take advantage of previously studied and feature engineered datasets to build **deep neural networks** (DNN) and **convolutional neural networks** (CNN) to predict the labels. We use [Tensorflow](https://www.tensorflow.org/) as backend framework and its _Keras_ module as working API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We first setup the environment and import relevant packages which we will use in the analysis. We print their versions to keep track of changes and set the **random seed** of all random generators in order to get reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7\n",
      "Matplot version: 3.2.1\n",
      "Numpy version: 1.18.1\n",
      "Pandas version: 1.0.3\n",
      "Scikit-learn version: 0.22.2.post1\n",
      "Scikit-optimize version: 0.7.4\n",
      "Tensorflow version: 2.0.0\n",
      "Keras version: 2.2.4-tf (backend: tensorflow)\n",
      "XGBoost version: 0.90\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random            as rnd\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import sklearn           as skl\n",
    "import skopt             as sko\n",
    "import tensorflow        as tf\n",
    "import xgboost           as xgb\n",
    "\n",
    "from tensorflow       import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning) # ignore UserWarning: I cannot really do anything about it...\n",
    "\n",
    "print('Python version: {:d}.{:d}'      .format(sys.version_info.major, sys.version_info.minor), flush=True)\n",
    "print('Matplot version: {}'            .format(mpl.__version__),                                flush=True)\n",
    "print('Numpy version: {}'              .format(np.__version__),                                 flush=True)\n",
    "print('Pandas version: {}'             .format(pd.__version__),                                 flush=True)\n",
    "print('Scikit-learn version: {}'       .format(skl.__version__),                                flush=True)\n",
    "print('Scikit-optimize version: {}'    .format(sko.__version__),                                flush=True)\n",
    "print('Tensorflow version: {}'         .format(tf.__version__),                                 flush=True)\n",
    "print('Keras version: {} (backend: {})'.format(keras.__version__, K.backend()),                 flush=True)\n",
    "print('XGBoost version: {}'            .format(xgb.__version__),                                flush=True)\n",
    "\n",
    "# fix random_seed\n",
    "RAND = 42\n",
    "rnd.seed(RAND)\n",
    "np.random.seed(RAND)\n",
    "tf.random.set_seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also print the **hardware specifications** to have a representation of the current build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  GNU/Linux - Arch Linux\n",
      "CPU: Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n",
      "RAM: 8 GiB\n",
      "GPU: NVIDIA Corporation GM108M [GeForce 940MX] (rev ff)\n"
     ]
    }
   ],
   "source": [
    "!echo \"OS:  $(uname -o) - $(lsb_release -d| sed 's/^.*:\\s*//g')\"\n",
    "!echo \"CPU: $(lscpu| grep 'Model name'| sed 's/^.*:\\s*//g')\"\n",
    "!echo \"RAM: $(free --giga| awk '/^Mem/ {print $7}') GiB\"\n",
    "!echo \"GPU: $(lspci | grep '3D controller' | sed 's/^.*controller:\\s*//g')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store information and results, we create a **logger** for the current Python session and a function to print information to the log file (or the standard output, if the logger is not defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from os import path, rename\n",
    "from time import strftime, gmtime\n",
    "\n",
    "def create_logfile(filename, name='logger', level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Create a logfile and rotate old logs.\n",
    "    \n",
    "    Required arguments:\n",
    "        filename: the name of the file or path to the log\n",
    "        \n",
    "    Optional arguments\n",
    "        name:     the name of the log session\n",
    "        level:    the level of the information stores\n",
    "        \n",
    "    Returns:\n",
    "        the log\n",
    "    \"\"\"\n",
    "    \n",
    "    # get current time to rename strings\n",
    "    ctime = strftime('.%Y%m%d.%H%M%S', gmtime())\n",
    "    \n",
    "    # rotate log if it already exists\n",
    "    if path.isfile(filename):\n",
    "        print('Rotating existing logs...', flush=True)\n",
    "        rename(filename, filename + ctime)\n",
    "    \n",
    "    # get a logging session by name\n",
    "    log = logging.getLogger(name + ctime)\n",
    "    log.setLevel(level)\n",
    "    \n",
    "    # define format\n",
    "    fmt = logging.Formatter('%(asctime)s --> %(levelname)s: %(message)s')\n",
    "    \n",
    "    # add the log file\n",
    "    han = logging.FileHandler(filename=filename)\n",
    "    han.setLevel(level)\n",
    "    han.setFormatter(fmt)\n",
    "    \n",
    "    # add handler for standard output\n",
    "    std = logging.StreamHandler(sys.stdout)\n",
    "    std.setLevel(level)\n",
    "    std.setFormatter(fmt)\n",
    "    \n",
    "    # create the output\n",
    "    log.addHandler(han)\n",
    "    log.addHandler(std)\n",
    "    \n",
    "    print('Created new log file!', flush=True)\n",
    "    return log\n",
    "\n",
    "def logprint(string, stream='info', logger=None):\n",
    "    \"\"\"\n",
    "    Decides whether to print on the logger or the standard output.\n",
    "    \n",
    "    Required arguments:\n",
    "        string: the string to print\n",
    "    \n",
    "    Optional arguments:\n",
    "        stream: standard input (info) or standard error (error)\n",
    "        logger: the logger (None for standard output/error)\n",
    "    \"\"\"\n",
    "    \n",
    "    if logger is not None:\n",
    "        if stream == 'info':\n",
    "            logger.info(string)\n",
    "        elif stream == 'error':\n",
    "            logger.error(string)\n",
    "        else:\n",
    "            logger.debug(string)\n",
    "    else:\n",
    "        if stream == 'info':\n",
    "            sys.stdout.write(string)\n",
    "        elif stream == 'error':\n",
    "            sys.stderr.write(string)\n",
    "        else:\n",
    "            sys.stdout.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Tools\n",
    "\n",
    "We first fetch the desired dataset and prepare the tools for the analysis. Specifically we need to:\n",
    "\n",
    "1. define the names of the main **directories** and create them if non existent,\n",
    "2. import the **database** and read the archive,\n",
    "3. create tools for **visualisation** and **manipulation** of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new log file!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from os import makedirs\n",
    "\n",
    "ROOT_DIR = '.'      # root directory\n",
    "IMG_DIR  = 'img'    # image directory\n",
    "MOD_DIR  = 'models' # directory of saved models\n",
    "LOG_DIR  = 'log'    # directory for logs\n",
    "\n",
    "# name of the dataset to be considered\n",
    "DB_NAME = 'cicy3o'\n",
    "DB_FILE = DB_NAME + '.h5'                                     # full name with extension\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE)                        # full path\n",
    "DB_DIR  = 'original' if DB_NAME == 'cicy3o' else 'favourable' # subdir where to store images, models, logs\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR, DB_DIR)\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR, DB_DIR)\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR, DB_DIR)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok=True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok=True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok=True)\n",
    "    \n",
    "# create logfile\n",
    "logger = create_logfile(filename=path.join(LOG_PATH, DB_NAME + '_nn.log'), name='CICY3', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then prepare to load the dataset and prepare for the visualisation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(filepath, mode='hdf5', shuffle=False, random_state=None, logger=None):\n",
    "    \"\"\"\n",
    "    Load a dataset given the path and the format.\n",
    "    \n",
    "    Required arguments:\n",
    "        filepath: the path of the file\n",
    "        \n",
    "    Optional arguments:\n",
    "        mode:         the format of the file\n",
    "        shuffle:      whether to shuffle the file\n",
    "        random_state: the seed of the random generator\n",
    "        logger:       the logging session (None for standard output)\n",
    "        \n",
    "    Returns:\n",
    "        the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if path.isfile(filepath):\n",
    "        logprint('Reading database...', logger=logger)\n",
    "        if mode == 'hdf5':\n",
    "            df = pd.read_hdf(filepath)\n",
    "        elif mode == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "        logprint('Database loaded!', logger=logger)\n",
    "    else:\n",
    "        logprint('Database is not available: cannot load the database!', stream='error', logger=logger)\n",
    "        \n",
    "    # shuffle the dataframe\n",
    "    if shuffle and random_state is not None:\n",
    "        logprint('Shuffling database...', logger=logger)\n",
    "        df = skl.utils.shuffle(df, random_state=random_state)\n",
    "        logprint('Database shuffled!', logger=logger)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define some functions we can use to extract and manipulate the database. We use the _Scikit-learn_ API to create _Estimator_ classes (inheriting the _Scikit_ interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# remove the outliers from a Pandas dataset\n",
    "class RemoveOutliers(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Remove outlying data given a dataset and a dictionary containing the intervals for each class.\n",
    "    \n",
    "    E.g.: if the two classes are 'h11' and 'h21', the dictionary will be: {'h11': [1, 16], 'h21': [1, 86]}.\n",
    "    \n",
    "    Public methods:\n",
    "        fit:           unused method\n",
    "        transform:     remove data outside the given interval\n",
    "        fit_transform: equivalent to transform(fit(...))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filter_dict=None):\n",
    "        \"\"\"\n",
    "        Constructor of the class.\n",
    "        \n",
    "        Optional arguments:\n",
    "            filter_dict: the intervals to retain in the data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.filter_dict = filter_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Unused method.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input by deleting data outside the interval\n",
    "        \n",
    "        Required arguments:\n",
    "            X: the dataset\n",
    "            \n",
    "        Returns:\n",
    "            the transformed dataset\n",
    "        \"\"\"\n",
    "\n",
    "        x = X.copy() # avoid overwriting\n",
    "\n",
    "        if self.filter_dict is not None:\n",
    "            for key in self.filter_dict:\n",
    "                x = x.loc[x[key] >= self.filter_dict[key][0]]\n",
    "                x = x.loc[x[key] <= self.filter_dict[key][1]]\n",
    "\n",
    "        return x\n",
    "\n",
    "# extract the tensors from a Pandas dataset\n",
    "class ExtractTensor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extract a dense tensor from sparse input from a given dataset.\n",
    "    \n",
    "    Public methods:\n",
    "        fit:           unused method\n",
    "        transform:     extract dense tensor\n",
    "        fit_transform: equivalent to transform(fit(...))\n",
    "        get_shape:     compute the shape of the tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, flatten=False, shape=None):\n",
    "        \"\"\"\n",
    "        Constructor of the class.\n",
    "        \n",
    "        Optional arguments:\n",
    "            flatten: whether to flatten the output or keep the current shape\n",
    "            shape:   force the computation with a given shape\n",
    "        \"\"\"\n",
    "\n",
    "        self.flatten = flatten\n",
    "        self.shape   = shape\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Unused method.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Compute the dense equivalent of the sparse input.\n",
    "        \n",
    "        Required arguments:\n",
    "            X: the dataset\n",
    "            \n",
    "        Returns:\n",
    "            the transformed input\n",
    "        \"\"\"\n",
    "\n",
    "        x = X.copy() # avoid overwriting\n",
    "        if self.shape is None:\n",
    "            self.shape = x.apply(np.shape).max() # get the shape of the tensor\n",
    "\n",
    "        if len(self.shape) > 0: # apply this to vectors and tensors\n",
    "            offset = lambda s : [ (0, self.shape[i] - np.shape(s)[i]) for i in range(len(self.shape)) ]\n",
    "            x      = x.apply(lambda s: np.pad(s, offset(s), mode='constant'))\n",
    "\n",
    "        if self.flatten and len(self.shape) > 0:\n",
    "            return list(np.stack(x.apply(np.ndarray.flatten).values))\n",
    "        else:\n",
    "            return list(np.stack(x.values))\n",
    "\n",
    "    def get_shape(self):\n",
    "        \"\"\"\n",
    "        Compute the shape of the tensor.\n",
    "        \n",
    "        Returns:\n",
    "            the shape of the tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the define the functions we will use to evaluate and improve the algorithms. Even though the ultimate goal is a regression task, we will however predict integer values $h_{11},~h_{21} \\in \\mathbb{Z}$, thus we will evaluate the **accuracy** of the prediction, given the best estimate (in general we use the _mean squared error_ to evaluate the algorithms, but we accept those with best _accuracy_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy (possibly after rounding)\n",
    "def accuracy_score(y_true, y_pred, rounding=np.rint):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the predictions after rounding.\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values\n",
    "        y_pred: predicted values\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    assert np.shape(y_true)[0] == np.shape(y_pred)[0] # check if same length\n",
    "    \n",
    "    # if same length then proceed\n",
    "    accuracy = 0\n",
    "    if rounding is not None:\n",
    "        for n in range(np.shape(y_true)[0]):\n",
    "            accuracy = accuracy + 1 \\\n",
    "                       if int(y_true[n]) == int(rounding(y_pred[n])) \\\n",
    "                       else accuracy\n",
    "    else:\n",
    "        for n in range(np.shape(y_true)[0]):\n",
    "            accuracy = accuracy + 1 \\\n",
    "                       if y_true[n] == y_pred[n] \\\n",
    "                       else accuracy\n",
    "    return accuracy / np.shape(y_true)[0]\n",
    "\n",
    "# get the error difference (possibly after rounding)\n",
    "def error_diff(y_true, y_pred, rounding=np.rint):\n",
    "    \"\"\"\n",
    "    Compute the error difference between true values and predictions (positive values are overestimate and viceversa).\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values\n",
    "        y_pred: predicted values\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    assert np.shape(y_true)[0] == np.shape(y_pred)[0] # check if same length\n",
    "\n",
    "    # if same length then proceed\n",
    "    err = y_true - rounding(y_pred)\n",
    "    return np.array(err).astype(np.int8)\n",
    "\n",
    "# print *SearchCV scores\n",
    "def gridcv_score(estimator, rounding=np.rint, logger=None):\n",
    "    \"\"\"\n",
    "    Print scores given by cross-validation and optimisation techniques.\n",
    "    \n",
    "    Required arguments:\n",
    "        estimator: the estimator to be evaluated\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "        logger:   the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "    \n",
    "    best_params = estimator.best_params_              # get best parameters\n",
    "    df          = pd.DataFrame(estimator.cv_results_) # dataframe with CV res.\n",
    "    \n",
    "    cv_best_res = df.loc[df['params'] == best_params] # get best results\n",
    "    accuracy    = cv_best_res.loc[:, 'mean_test_score'].values[0]\n",
    "    std         = cv_best_res.loc[:, 'std_test_score'].values[0]\n",
    "    \n",
    "    logprint('Best parameters: {}'.format(best_params), logger=logger)\n",
    "    logprint('Accuracy ({}) of cross-validation: ({:.3f} ± {:.3f})%'.format(rounding.__name__, accuracy*100, std*100), logger=logger)\n",
    "    \n",
    "# print the accuracy of the predictions\n",
    "def prediction_score(estimator, X, y, use_best_estimator=False, rounding=np.rint, logger=None):\n",
    "    \"\"\"\n",
    "    Print the accuracy of the predictions.\n",
    "    \n",
    "    Required arguments:\n",
    "        estimator: the estimator to be used for the predictions\n",
    "        X: the features\n",
    "        y: the labels (actual values)\n",
    "        \n",
    "    Optional arguments:\n",
    "        use_best_estimator: whether to use the estimator.best_estimator_ or just estimator\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "        logger:   the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_best_estimator:\n",
    "        estimator = estimator.best_estimator_\n",
    "    \n",
    "    accuracy = accuracy_score(y, estimator.predict(X), rounding=rounding)\n",
    "    logprint('Accuracy ({}) of the predictions: {:.3f}%'.format(rounding.__name__, accuracy*100), logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use _Matplotlib_ to plot the data and define a few functions which we can use during the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set label sizes\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=12)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# set building block sizes for the plot\n",
    "mpl_width  = 6\n",
    "mpl_height = 5\n",
    "\n",
    "# save the current figure\n",
    "def save_fig(filename, tight_layout=True, extension='png', resolution=96, logger=None):\n",
    "    \"\"\"\n",
    "    Save current figure to file.\n",
    "    \n",
    "    Required arguments:\n",
    "        filename: the name of the file where to save the figure (without extension)\n",
    "        \n",
    "    Optional arguments:\n",
    "        tight_layout: whether to use the tight_layout\n",
    "        extension:    extension of the file to use\n",
    "        resolution:   resolution of the file\n",
    "        logger:       the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "\n",
    "    filename = path.join(IMG_PATH, filename + '.' + extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "\n",
    "    logprint('Saving {}...'.format(filename), logger=logger)\n",
    "    plt.savefig(filename, format=extension, dpi=resolution)\n",
    "    logprint('Saved {}!'.format(filename), logger=logger)\n",
    "\n",
    "# get a generator to count the occurrencies\n",
    "def get_counts(df, label, feature):\n",
    "    \"\"\"\n",
    "    Generator to produce the count of unique occurrencies of the data.\n",
    "    \n",
    "    Required arguments:\n",
    "        df:      the Pandas dataframe\n",
    "        label:   the label to consider\n",
    "        feature: the feature to consider\n",
    "        \n",
    "    Yields:\n",
    "        [ unique feature, unique value, counts ]\n",
    "    \"\"\"\n",
    "\n",
    "    for n in np.sort(df[feature].unique()):\n",
    "        uniques, counts = np.unique(df[label].loc[df[feature] == n].values, return_counts=True)\n",
    "        for u, c in np.c_[uniques, counts]:\n",
    "            yield [ n, u, c ]\n",
    "\n",
    "# plot histogram of occurrencies\n",
    "def count_plot(ax, data, title=None, xlabel=None, ylabel='N',\n",
    "               legend=None, xlog=False, ylog=False, binstep=5,\n",
    "               **kwargs):\n",
    "    \"\"\"\n",
    "    Plot histogram of occurrencies (e.g.: frequency plot).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        binstep:  the distance between adjacent bins\n",
    "        **kwargs: additional arguments to pass to plt.hist\n",
    "    \"\"\"\n",
    "\n",
    "    min_tick = np.min(data) if np.min(data) > -100 else -100 # set a MIN cut\n",
    "    max_tick = np.max(data) if np.max(data) < 100  else 100  # set a MAX cut\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create a grid\n",
    "    ax.set_title(title)                  # set title\n",
    "    ax.set_xlabel(xlabel)                # set a label for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set a label for the y axis\n",
    "    ax.set_xticks(np.arange(min_tick,    # set no. of ticks in the x axis\n",
    "                            max_tick,\n",
    "                            step=binstep\n",
    "                           )\n",
    "                 )\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    ax.hist(data,                        # create histogram using 'step' funct.\n",
    "            histtype='step',\n",
    "            label=legend,\n",
    "            **kwargs)\n",
    "\n",
    "    if legend is not None:               # add legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot labeled features and their values\n",
    "def label_plot(ax, data, title=None, xlabel=None, ylabel='values',\n",
    "               legend=None, xlog=False, ylog=False, binstep=1,\n",
    "               **kwargs):\n",
    "    \"\"\"\n",
    "    Plot values of labelled data (e.g.: variable ranking).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        binstep:  the distance between adjacent bins\n",
    "        **kwargs: additional arguments to pass to plt.plot\n",
    "    \"\"\"\n",
    "\n",
    "    labels      = [f[0] for f in data]   # labels vector\n",
    "    importances = [f[1] for f in data]   # importances vector\n",
    "    length      = len(labels)            # length of the labels vector\n",
    "    \n",
    "    ax.grid(alpha=0.2)                   # create a grid\n",
    "    ax.set_title(title)                  # set title\n",
    "    ax.set_xlabel(xlabel)                # set a label for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set a label for the x axis\n",
    "\n",
    "    ax.set_xticks(np.arange(length,      # set no. of ticks in the x axis\n",
    "                            step=binstep\n",
    "                           )\n",
    "                 )\n",
    "    ax.set_xticklabels(labels,           # set name of labels of the x axis\n",
    "                       ha='right',       # horizontal alignment\n",
    "                       rotation=45       # rotation of the labels\n",
    "                      )\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    ax.plot(np.arange(length),           # plot data\n",
    "            importances,\n",
    "            label=legend,\n",
    "            **kwargs)\n",
    "\n",
    "    if legend is not None:               # add legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot the correlation matrix of a Pandas dataframe\n",
    "def mat_plot(ax, matrix, labels, label='correlation matrix', **kwargs):\n",
    "    \"\"\"\n",
    "    Plot the correlation matrix of a given dataframe.\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:     the subplot ax where to plot data\n",
    "        matrix: the matrix to plot\n",
    "        labels: the labels to show with the matrix\n",
    "        \n",
    "    Optional arguments:\n",
    "        label:    the label to use for the colour bar\n",
    "        **kwargs: additional arguments to pass to plt.matshow\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(labels), # set ticks for x axis\n",
    "                  step=1)\n",
    "                 )\n",
    "    ax.set_xticklabels([''] + labels,    # set the name of the ticks\n",
    "                       rotation=90\n",
    "                      )\n",
    "\n",
    "    ax.set_yticks(np.arange(len(labels), # set ticks for y axis\n",
    "                  step=1)\n",
    "                 )\n",
    "    ax.set_yticklabels([''] + labels)    # set the name of the ticks\n",
    "                    \n",
    "    matshow = ax.matshow(matrix,         # show the matrix\n",
    "                         vmin=-1.0,\n",
    "                         vmax=1.0,\n",
    "                         **kwargs\n",
    "                        )\n",
    "                                \n",
    "    cbar = ax.figure.colorbar(matshow,   # create the colour bar\n",
    "                              ax=ax,\n",
    "                              fraction=0.05,\n",
    "                              pad=0.05\n",
    "                             )\n",
    "    cbar.ax.set_ylabel(label,            # show the colour bar\n",
    "                       va='bottom',      # vertical alignment\n",
    "                       rotation=-90)     # rotation of the label\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot a scatter plot with colours and sizes\n",
    "def scatter_plot(ax, data, title=None, xlabel=None, ylabel=None,\n",
    "                 legend=None, xlog=False, ylog=False,\n",
    "                 colour=True, size=True, colour_label='N', size_leg=0,\n",
    "                 **kwargs):\n",
    "    \"\"\"\n",
    "    Scatter plot of occurrencies with colour and size codes.\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:        the title of the plot\n",
    "        xlabel:       the label of the x axis\n",
    "        ylabel:       the label of the y axis\n",
    "        legend:       the label for the legend in the plot\n",
    "        xlog:         whether to use the log scale on the x axis\n",
    "        ylog:         whether to use the log scale on the y axis\n",
    "        colour:       whether to use colour codes\n",
    "        size:         whether to use entries of different size\n",
    "        colour_label: label to use for the colour code\n",
    "        size_leg:     length of the legend of the size code\n",
    "        **kwargs:     additional arguments to pass to plt.scatter\n",
    "    \"\"\"\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create  a grid\n",
    "    ax.set_xlabel(xlabel)                # set labels for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set labels for the y axis\n",
    "    ax.set_title(title)                  # set title\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    if colour:                           # create the plot with size and colours\n",
    "        if size:\n",
    "            scat = ax.scatter(data[0], data[1], s=data[2], c=data[2], **kwargs)\n",
    "        else:\n",
    "            scat = ax.scatter(data[0], data[1], c=data[2], **kwargs)\n",
    "        cbar = ax.figure.colorbar(scat, ax=ax)\n",
    "        cbar.ax.set_ylabel(colour_label, rotation=-90, va='bottom')\n",
    "    else:\n",
    "        if size:\n",
    "            scat = ax.scatter(data[0], data[1], s=data[2], **kwargs)\n",
    "        else:\n",
    "            scat = ax.scatter(data[0], data[1], **kwargs)\n",
    "\n",
    "    scat.set_label(legend)               # set label of the plot\n",
    "    if size_leg:                         # add the size legend if needed\n",
    "        handles, labels = scat.legend_elements('sizes', num=size_leg)\n",
    "        ax.legend(handles, labels, loc='lower center',\n",
    "                  bbox_to_anchor=(0.5,-0.3), ncol=len(handles),\n",
    "                  fontsize='medium', frameon=False)\n",
    "\n",
    "    if legend:                           # show the legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot a series with trivial x label\n",
    "def series_plot(ax, data, title=None, xlabel='series', ylabel=None,\n",
    "                legend=None, xlog=False, ylog=False,\n",
    "                step=False, std=False,\n",
    "                **kwargs):\n",
    "    \"\"\"\n",
    "    Plot a series of data with ordered x axis (e.g.: epoch series).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        step:     whether to use a step function for the plot\n",
    "        std:      highlight the strip of the standard deviation\n",
    "        **kwargs: additional arguments to pass to plt.step or plot.plot\n",
    "    \"\"\"\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create the grid\n",
    "    ax.set_title(title)                  # set the title\n",
    "    ax.set_xlabel(xlabel)                # set labels for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set labels for the y axis\n",
    "\n",
    "    if xlog:                             # use log scale in the x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in the y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    series = np.arange(len(data))        # create trivial x axis data\n",
    "    if step:                             # create the plot\n",
    "        ax.step(series, data, label=legend, **kwargs)\n",
    "    else:\n",
    "        ax.plot(series, data, label=legend, **kwargs)\n",
    "\n",
    "    if std:                              # show coloured strip with std\n",
    "        ax.fill_between(series,\n",
    "                        data + np.std(data),\n",
    "                        data - np.std(data),\n",
    "                        alpha=0.2)\n",
    "\n",
    "    if legend is not None:               # show the legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further we also set the **memory growth** of the GPU RAM in order to avoid memory issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-09 09:30:33,266 --> ERROR: No GPUs in the setup!\n"
     ]
    }
   ],
   "source": [
    "# get the list of installed GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) # set memory growth\n",
    "            \n",
    "        # get the list of logical devices (GPUs)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        logprint('GPU setup: {:d} physical GPUs, {:d} logical GPUs.'.format(len(gpus), len(logical_gpus)), logger=logger)\n",
    "    except RuntimeError as e:\n",
    "        logprint(e, stream='error', logger=logger)\n",
    "else:\n",
    "    logprint('No GPUs in the setup!', stream='error', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "We then proceed with the analysis by first loading the prepared datasets and then building the appropriate architectures for the prediction of the Hodge numbers. We start from the analysis of the configuration matrix through a **CNN** architecture and then compare the results with the analysis of the engineered dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Matrix Only\n",
    "\n",
    "We first consider only the configuration matrix and build a CNN to predict the labels. First of all we load the dataset and then divide labels and features, we then proceed with the architecture of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-09 09:40:09,455 --> INFO: Reading database...\n",
      "2020-04-09 09:40:09,512 --> INFO: Database loaded!\n"
     ]
    }
   ],
   "source": [
    "if path.isfile(path.join(ROOT_DIR, DB_NAME + '_matrix.h5')):\n",
    "    matrix = load_dataset(path.join(ROOT_DIR, DB_NAME + '_matrix.h5'), logger=logger)\n",
    "    h11    = ExtractTensor(flatten=False).fit_transform(matrix['h11'])\n",
    "    h21    = ExtractTensor(flatten=False).fit_transform(matrix['h21'])\n",
    "    matrix = ExtractTensor(flatten=False).fit_transform(matrix['matrix'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
