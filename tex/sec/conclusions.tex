From the result of the analysis it seems that a regression task well suits the dataset and it is therefore possible to extract meaningful information from it. A classification task would have meant to manually create boundaries to the predictions of the Hodge numbers, thus defying any effort to find a relation among the features. Furthermore the final algorithm would be unable to predict any outlier with respect to the training dataset.

In particular the algorithms from the \textit{Scikit} library show that linear regression can prove useful mainly when integrated to a $l1$ regularisation, while the ``more geometrical'' support vectors usually give a better result, even better than the random forests of decision trees. The NNs however are a bit more subtle to train and it seems that the best practice is to leave the task of finding a good representation of the features to the network itself (through a CNN for example) instead of preparing a processed dataset as in the previous case. In any case, it is always best to use kernel transformations through convolution than using a DNN as in most image recognition applications \cite{NIPS2012_4824}. However since in general we want to retain as much information as possible, the characteristic \texttt{MaxPooling} operation does not help here (results are usually worse). Also the kernel size used is larger than typical applications, but this could be related to the fact that we are dealing with mostly sparse matrices with a lot of redundancy and the relevant information is entirely hidden just in a particular subset of the entries \cite{NIPS2014_5544,jaderberg2014speeding,Liu_2015_CVPR}. Moreover, feeding the network with more engineered features helps only in the prediction of $h_{21}$, while the prediction of $h_{11}$ suffers from such an addition: looking at the data distribution it seems that $h_{11}$ is linearly connected to most of the features that the network is able to autonomously extract from the configuration matrix, while the prediction of $h_{21}$ benefits from the reinforcement of some features (similar in a sense to a \textit{skip-connection} in a RNN).

Ensemble learning of multiple algorithms can help in getting good predictions. Even though we used less data to train the first level algorithms with a consequent loss of accuracy in the test predictions (e.g.: the sequential CNN on $h_{11}$ achieved $90\%$ of accuracy with $80\%$ of the dataset for training, while it decreased to $89\%$ using $50\%$ of it), we were however able to recover the previous result for $h_{11}$ by stacking the algorithms. The results for $h_{21}$ were however way better than the single algorithm analysis, improving the prediction by as much as $4\%$ on the best result of the previous analysis. The predictions for $h_{11}$ also gained $1\%$ in accuracy.

Finally we also showed that feature engineering is extremely helpful in improving the results, especially when the used dataset is not very large (e.g. in the stacking ensemble we use less samples than before, but the accuracy of the ML algorithm is comparable to the previous analysis in the case of the engineered dataset and the CNN with some engineered features even outperforms the CNN without them).

As a general conclusion we can say:
\begin{itemize}
    \item Feature engineering can help in getting better predictions. For instance, using the configuration matrix alone we get roughly $12\%$ of accuracy on the predictions of $h_{21}$ for almost all algorithms (not the NN, but we already discussed them), while the feature engineered set achieves improved results (almost three times better in most cases).
    \item Stacking ensemble learning can help a lot in getting better predictions. For example, the results for $h_{11}$ show an improvement  of $2\%$ with respect to the best results of the NN and almost $20\%$ better than the best \textit{Scikit} algorithm.
    \item Fully connected layers are good at transforming already processed inputs and finding possible new representations of the data, but processing it through convolutions helps in learning new usable data which can then be processed through FC networks.
\end{itemize}