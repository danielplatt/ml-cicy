Include Neural Networks and ensemble learning (stacking)\footnote{I decided to go one step back and implement a better architecture for the networks, thus I only have some preliminary results using the old architecture. For instance with a better (and quite longer analysis) I was able to improve also the \texttt{Sequential} models and reach 88\% of accuracy for $h_{11}$ and 36\% for $h_{21}$.}: even though they still have to be better implemented, the \texttt{Sequential} model in \textit{Keras} can reach accuracy above 84\% for $h_{11}$ but still below 40\% for $h_{21}$ while its \texttt{Functional} API counterpart (using \texttt{Conv2D}) slightly improves the results but not dramatically. The \texttt{Conv1D} and \texttt{Dense} models lead to much worse results. Using the stacking learning (naively with all previous algorithms stacked together\footnote{It  might be a good idea to drop the \texttt{Conv1D} and \texttt{Dense} models on the \texttt{PCA} components, since their results are visibly worse on the entire range of $h_{11}$ and $h_{21}$: I do not expect them to help much since the other algorithms produce better predictions in the entire range.}) it is however possible to improve the results reaching almost 90\% for $h_{11}$ and 43\% for $h_{21}$.